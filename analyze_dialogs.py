# analyze_dialogs.py
import json
import config
from pathlib import Path
from striprtf.striprtf import rtf_to_text
import ollama # –î–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–ª–∏ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ---
# –û–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ—Ä–æ–≥–∏ –¥–ª–∏–Ω—ã –≤ —Ç–æ–∫–µ–Ω–∞—Ö
LENGTH_THRESHOLDS = [512, 1024, 2048, 4096]

def count_tokens_approx(text):
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤."""
    # –û—á–µ–Ω—å –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ~= 4 —Å–∏–º–≤–æ–ª–∞
    return len(text) / 4

def analyze():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Ö –¥–ª–∏–Ω–µ."""
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–∞–ø–∫–∞–º...")
    
    total_chars = 0
    total_tokens_approx = 0
    dialog_count = 0
    token_counts = []
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø–æ —Ç–µ–º–∞–º
    theme_stats = {}

    # --- –ù–û–í–û–ï: –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø–æ—Ä–æ–≥–æ–≤ ---
    threshold_counts = {thresh: 0 for thresh in LENGTH_THRESHOLDS}
    above_max_threshold = 0 # –°—á–µ—Ç—á–∏–∫ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –ø–æ—Ä–æ–≥–∞

    for theme_folder in config.INPUT_ROOT.iterdir():
        if theme_folder.is_dir():
            theme_name = theme_folder.name
            print(f"  üìÇ –ê–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏ —Ç–µ–º—ã: {theme_name}")
            
            theme_dialog_count = 0
            theme_total_tokens = 0
            theme_token_counts = []

            for file in theme_folder.glob("*.rtf"):
                try:
                    raw_text = rtf_to_text(file.read_text(encoding="utf-8", errors="ignore"))
                    chars = len(raw_text)
                    tokens = count_tokens_approx(raw_text)
                    
                    total_chars += chars
                    total_tokens_approx += tokens
                    token_counts.append(tokens)
                    dialog_count += 1
                    
                    theme_dialog_count += 1
                    theme_total_tokens += tokens
                    theme_token_counts.append(tokens)
                    
                    # --- –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤ ---
                    for thresh in LENGTH_THRESHOLDS:
                        if tokens <= thresh:
                            threshold_counts[thresh] += 1
                    if tokens > LENGTH_THRESHOLDS[-1]: # –î–ª–∏–Ω–Ω–µ–µ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –ø–æ—Ä–æ–≥–∞
                         above_max_threshold += 1

                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file}: {e}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–µ
            if theme_dialog_count > 0:
                theme_stats[theme_name] = {
                    "count": theme_dialog_count,
                    "avg_tokens": theme_total_tokens / theme_dialog_count,
                    "max_tokens": max(theme_token_counts),
                    "min_tokens": min(theme_token_counts),
                }

    # --- –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    if dialog_count > 0:
        avg_chars = total_chars / dialog_count
        avg_tokens = total_tokens_approx / dialog_count
        max_tokens = max(token_counts)
        min_tokens = min(token_counts)
        
        print(f"\n--- –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –¥–∏–∞–ª–æ–≥–∞–º ---")
        print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {dialog_count}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {avg_chars:.2f}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {avg_tokens:.2f}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {max_tokens:.2f}")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {min_tokens:.2f}")
        
        if max_tokens > LENGTH_THRESHOLDS[0]:
            print(f"\n--- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º ---")
            print(f"(–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ—Ä–æ–≥–∏: {LENGTH_THRESHOLDS})")
            for thresh in LENGTH_THRESHOLDS:
                percentage = (threshold_counts[thresh] / dialog_count) * 100
                print(f"  ‚è±Ô∏è  –î–∏–∞–ª–æ–≥–æ–≤ —Å <= {thresh} —Ç–æ–∫–µ–Ω–æ–≤: {threshold_counts[thresh]} ({percentage:.2f}%)")
            if above_max_threshold > 0:
                percentage_above = (above_max_threshold / dialog_count) * 100
                print(f"  ‚è±Ô∏è  –î–∏–∞–ª–æ–≥–æ–≤ —Å > {LENGTH_THRESHOLDS[-1]} —Ç–æ–∫–µ–Ω–æ–≤: {above_max_threshold} ({percentage_above:.2f}%)")
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
            print(f"\n--- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ---")
            # –ù–∞–π–¥–µ–º –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫—Ä—ã–≤–∞–µ—Ç >= 95% –¥–∏–∞–ª–æ–≥–æ–≤
            target_coverage = 95.0
            recommended_length = None
            for thresh in sorted(LENGTH_THRESHOLDS, reverse=True):
                percentage = (threshold_counts[thresh] / dialog_count) * 100
                if percentage >= target_coverage:
                    recommended_length = thresh
            
            if recommended_length:
                print(f"  ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è MODEL.max_seq_length = {recommended_length} (–ø–æ–∫—Ä—ã–≤–∞–µ—Ç ~{target_coverage}% –¥–∏–∞–ª–æ–≥–æ–≤).")
            else:
                # –ï—Å–ª–∏ –¥–∞–∂–µ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –ø–æ—Ä–æ–≥ –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç 95%
                if above_max_threshold > 0:
                    percentage_above = (above_max_threshold / dialog_count) * 100
                    print(f"  ‚ö†Ô∏è  –ù–µ—Ç –ø–æ—Ä–æ–≥–∞, –ø–æ–∫—Ä—ã–≤–∞—é—â–µ–≥–æ {target_coverage}%. {percentage_above:.2f}% –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ {LENGTH_THRESHOLDS[-1]} —Ç–æ–∫–µ–Ω–æ–≤.")
                    print(f"  üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ MODEL.max_seq_length = {LENGTH_THRESHOLDS[-1]} –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤.")
                else:
                     # –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ —É–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                     print(f"  ‚úÖ MODEL.max_seq_length = {LENGTH_THRESHOLDS[-1]} –ø–æ–∫—Ä–æ–µ—Ç 100% –¥–∏–∞–ª–æ–≥–æ–≤.")
                     
        else:
            print(f"\n‚úÖ –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ –∫–æ—Ä–æ—á–µ {LENGTH_THRESHOLDS[0]} —Ç–æ–∫–µ–Ω–æ–≤. MODEL.max_seq_length = 512 –±—É–¥–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º.")
            
        # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º ---
        if theme_stats:
            print(f"\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º ---")
            for theme, stats in theme_stats.items():
                print(f"  üìÅ {theme}:")
                print(f"    - –§–∞–π–ª–æ–≤: {stats['count']}")
                print(f"    - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ (—Ç–æ–∫–µ–Ω—ã): {stats['avg_tokens']:.2f}")
                print(f"    - –ú–∏–Ω/–ú–∞–∫—Å –¥–ª–∏–Ω–∞: {stats['min_tokens']:.2f} / {stats['max_tokens']:.2f}")

    else:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ .rtf —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

if __name__ == "__main__":
    analyze()