=== PROJECT STRUCTURE ===
[FILE] aggregate_phrases.py
[FILE] analysis_methods.py
[FILE] analyze_dialogs.py
[DIR]  cache
[FILE] classifier.py
[FILE] config.py
[DIR]  data
[DIR]  db
[FILE] export_phrases.py
[DIR]  exports
[DIR]  faiss_index
[FILE] generate_callback_phrases.py
[FILE] generate_callback_phrases_cloud_db.py
[FILE] generate_callback_phrases_internal_llm.py
[FILE] generate_callback_phrases_multi_model.py
[FILE] gui.py
[FILE] indexer.py
[FILE] init_db.py
[DIR]  Input
[DIR]  Input\CallBack
[DIR]  Input\–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ_–¥–æ–≥–æ–≤–æ—Ä–∞
[DIR]  logs
[FILE] pack_project.py
[FILE] phrase_stats.py
[FILE] pipeline.py
[DIR]  processed
[DIR]  processed\CallBack
[DIR]  processed\–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ_–¥–æ–≥–æ–≤–æ—Ä–∞
[FILE] retry_failed_batches.py
[DIR]  temp
[FILE] utils.py

=== PYTHON FILES CONTENT ===

===== FILE: aggregate_phrases.py =====
import json
from collections import defaultdict

# –ó–∞–≥—Ä—É–∑–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –æ–Ω–∏ –≤ —Ñ–∞–π–ª–µ 'callback_results.json')
with open('callback_results.json', 'r', encoding='utf-8') as f:
    results = json.load(f)

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
aggregated = {
    "category_1_phrases": {"client": [], "operator": []},
    "category_2_phrases": {"client": [], "operator": []},
    "category_3_phrases": {"client": [], "operator": []},
    "category_4_phrases": {"client": [], "operator": []}
}

for item in results:
    cat = item["category"]
    key = f"category_{cat}_phrases"
    aggregated[key]["client"].extend(item["client_phrases"])
    aggregated[key]["operator"].extend(item["operator_phrases"])

# –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
for cat_key in aggregated:
    for speaker in ["client", "operator"]:
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_phrases = []
        for phrase in aggregated[cat_key][speaker]:
            if phrase not in seen:
                seen.add(phrase)
                unique_phrases.append(phrase)
        aggregated[cat_key][speaker] = sorted(unique_phrases, key=str.lower)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open('aggregated_phrases.json', 'w', encoding='utf-8') as f:
    json.dump(aggregated, f, ensure_ascii=False, indent=2)

print("‚úÖ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç: aggregated_phrases.json")

===== FILE: analysis_methods.py =====
# analysis_methods.py
"""
–ú–æ–¥—É–ª—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤.
–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –æ–Ω –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ gui.py.
"""

import logging
import ollama
import time
import json

# === –ò–º–ø–æ—Ä—Ç –¥–ª—è fast_phrase_classifier ===
from classifier import classify_dialog_with_phrases

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

OLLAMA_MODEL_NAME = "dimweb/ilyagusev-saiga_llama3_8b:kto_v5_Q4_K"


# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ Ollama —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ ===
def call_ollama_with_retry(prompt, model_name=OLLAMA_MODEL_NAME, max_retries=3, delay=1):
    """
    –í—ã–∑—ã–≤–∞–µ—Ç Ollama —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
    """
    for attempt in range(max_retries):
        try:
            logger.info(f"–í—ã–∑–æ–≤ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries})...")
            response = ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                options={
                    "num_ctx": 4096,
                    "num_gpu": -1
                }
            )
            time.sleep(delay)
            return response["message"]["content"]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(delay * 2)
            else:
                raise e


# === –ü–æ–¥—Ö–æ–¥ 1: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π (–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π) –∞–Ω–∞–ª–∏–∑ ===
def hierarchical_analysis(question, found_with_scores, chunk_size=10, status_callback=None):
    try:
        total_found = len(found_with_scores)
        if status_callback:
            status_callback(f"üß† –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

        summaries = []
        num_chunks = (total_found + chunk_size - 1) // chunk_size
        for i in range(0, len(found_with_scores), chunk_size):
            chunk_num = i // chunk_size + 1
            if status_callback:
                status_callback(f"üß† –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã {chunk_num}/{num_chunks}...")

            chunk = found_with_scores[i:i + chunk_size]
            chunk_context_parts = []
            for item, score in chunk:
                text_snippet = item['text'][:800] + ("..." if len(item['text']) > 800 else "")
                chunk_context_parts.append(f"[–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}] ID: {item['id']}\n{text_snippet}")
            
            chunk_context = "\n---\n".join(chunk_context_parts)
            chunk_prompt = (
                f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –∫—Ä–∞—Ç–∫–æ –æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
                f"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –æ—Ç–≤–µ—Ç—å—Ç–µ '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'. –û—Ç–≤–µ—á–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.\n"
                f"–í–æ–ø—Ä–æ—Å: {question}\n"
                f"–§—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤:\n{chunk_context}\n"
                f"–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç:"
            )
            
            try:
                summary = call_ollama_with_retry(chunk_prompt)
                summaries.append(f"–ì—Ä—É–ø–ø–∞ {chunk_num} (–¥–∏–∞–ª–æ–≥–∏ {i+1}-{min(i+chunk_size, total_found)}):\n{summary}")
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –≥—Ä—É–ø–ø–∞ {chunk_num}/{num_chunks}")
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä—É–ø–ø—ã {chunk_num}: {e}"
                logger.error(error_msg)
                summaries.append(f"–ì—Ä—É–ø–ø–∞ {chunk_num}: –û–®–ò–ë–ö–ê - {str(e)}")

        if not summaries:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."

        if status_callback:
            status_callback("üß† –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è...")

        final_context = "\n\n".join(summaries)
        final_prompt = (
            f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –≥—Ä—É–ø–ø–∞–º –¥–∏–∞–ª–æ–≥–æ–≤, –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –∏ –¥–∞–π—Ç–µ –æ–±—â–∏–π, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. "
            f"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏—Ç–µ –æ–± —ç—Ç–æ–º.\n"
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n"
            f"–û—Ç–≤–µ—Ç—ã –ø–æ –≥—Ä—É–ø–ø–∞–º:\n{final_context}\n"
            f"–û–±—â–∏–π –æ—Ç–≤–µ—Ç:"
        )
        
        try:
            final_answer = call_ollama_with_retry(final_prompt)
        except Exception as e:
            final_answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {e}\n\n–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n{final_context}"

        context_text = f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {total_found} –¥–∏–∞–ª–æ–≥–æ–≤.\n\n"
        for item, score in found_with_scores:
            context_text += f"ID: {item['id']}\n–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}\n–¢–µ–∫—Å—Ç: {item['text'][:300]}...\n---\n"

        return final_answer, context_text

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≤ hierarchical_analysis: {e}"
        logger.error(error_msg)
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}", f"–û—à–∏–±–∫–∞: {error_msg}"


# === –ü–æ–¥—Ö–æ–¥ 2: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ (Rolling Summary) ===
def rolling_summary_analysis(question, found_with_scores, chunk_size=5, status_callback=None):
    try:
        total_found = len(found_with_scores)
        if status_callback:
            status_callback(f"üß† –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

        summary = "–ù–∞—á–∞–ª—å–Ω—ã–π –∏—Ç–æ–≥ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."
        processed_count = 0

        for i in range(0, len(found_with_scores), chunk_size):
            if status_callback:
                status_callback(f"üß† –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i+chunk_size, total_found)}/{total_found}...")

            chunk = found_with_scores[i:i + chunk_size]
            chunk_context_parts = []
            for item, score in chunk:
                text_snippet = item['text'][:500] + ("..." if len(item['text']) > 500 else "")
                chunk_context_parts.append(f"[–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}] ID: {item['id']}\n{text_snippet}")
            
            chunk_context = "\n---\n".join(chunk_context_parts)
            
            prompt = (
                f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. "
                f"–í–æ–ø—Ä–æ—Å: {question}\n"
                f"–¢–µ–∫—É—â–∏–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥: {summary}\n"
                f"–ù–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏ –¥–ª—è —É—á–µ—Ç–∞:\n{chunk_context}\n"
                f"–û–±–Ω–æ–≤–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥, —É—á–∏—Ç—ã–≤–∞—è –Ω–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ."
            )
            
            try:
                summary = call_ollama_with_retry(prompt)
                processed_count += len(chunk)
                logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω –∏—Ç–æ–≥ –ø–æ—Å–ª–µ {processed_count} –¥–∏–∞–ª–æ–≥–æ–≤")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏—Ç–æ–≥–∞: {e}")
                summary += f"\n[–û—à–∏–±–∫–∞ –Ω–∞ —à–∞–≥–µ {processed_count//chunk_size + 1}: {e}]"

        final_prompt = (
            f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –∏—Ç–æ–≥–∞, –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n"
            f"–í–æ–ø—Ä–æ—Å: {question}\n"
            f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥: {summary}\n"
            f"–û—Ç–≤–µ—Ç:"
        )
        
        if status_callback:
            status_callback("üß† –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ: —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞...")

        try:
            final_answer = call_ollama_with_retry(final_prompt)
        except Exception as e:
            final_answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ: {e}\n\n–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥:\n{summary}"

        context_text = f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_found} –¥–∏–∞–ª–æ–≥–æ–≤ –º–µ—Ç–æ–¥–æ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.\n\n"
        for item, score in found_with_scores:
            context_text += f"ID: {item['id']}\n–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}\n–¢–µ–∫—Å—Ç: {item['text'][:300]}...\n---\n"

        return final_answer, context_text

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≤ rolling_summary_analysis: {e}"
        logger.error(error_msg)
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}", f"–û—à–∏–±–∫–∞: {error_msg}"


# === –ü–æ–¥—Ö–æ–¥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ (Information Extraction) ===
def fact_extraction_analysis(question, found_with_scores, chunk_size=10, status_callback=None):
    try:
        total_found = len(found_with_scores)
        if status_callback:
            status_callback(f"üß† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

        all_facts = []
        num_chunks = (total_found + chunk_size - 1) // chunk_size
        for i in range(0, len(found_with_scores), chunk_size):
            chunk_num = i // chunk_size + 1
            if status_callback:
                status_callback(f"üß† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã {chunk_num}/{num_chunks}...")

            chunk = found_with_scores[i:i + chunk_size]
            chunk_context_parts = []
            for item, score in chunk:
                text_snippet = item['text'][:1000] + ("..." if len(item['text']) > 1000 else "")
                chunk_context_parts.append(f"[–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}] ID: {item['id']}\n{text_snippet}")
            
            chunk_context = "\n---\n".join(chunk_context_parts)
            
            prompt = (
                f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤, "
                f"–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
                f"–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: —Å–ø–∏—Å–æ–∫ –ø—É–Ω–∫—Ç–æ–≤, –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç - –æ–¥–∏–Ω —Ñ–∞–∫—Ç. "
                f"–ï—Å–ª–∏ –≤ –¥–∏–∞–ª–æ–≥–µ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–∞–ø–∏—à–∏ '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'.\n"
                f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n"
                f"–î–∏–∞–ª–æ–≥–∏:\n{chunk_context}\n"
                f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã:"
            )
            
            try:
                facts = call_ollama_with_retry(prompt)
                all_facts.append(f"–ì—Ä—É–ø–ø–∞ {chunk_num}:\n{facts}")
                logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã —Ñ–∞–∫—Ç—ã –∏–∑ –≥—Ä—É–ø–ø—ã {chunk_num}/{num_chunks}")
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã {chunk_num}: {e}"
                logger.error(error_msg)
                all_facts.append(f"–ì—Ä—É–ø–ø–∞ {chunk_num}: –û–®–ò–ë–ö–ê - {str(e)}")

        if not all_facts:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ñ–∞–∫—Ç—ã.", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."

        if status_callback:
            status_callback("üß† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑...")

        facts_summary = "\n\n".join(all_facts)
        analysis_prompt = (
            f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. "
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–∏ —Ñ–∞–∫—Ç—ã –∏ –¥–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. "
            f"–ü–æ–¥—Å—á–∏—Ç–∞–π—Ç–µ —á–∞—Å—Ç–æ—Ç—ã, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ. "
            f"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏—Ç–µ –æ–± —ç—Ç–æ–º.\n"
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n"
            f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã:\n{facts_summary}\n"
            f"–ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç–≤–µ—Ç:"
        )
        
        try:
            final_answer = call_ollama_with_retry(analysis_prompt)
        except Exception as e:
            final_answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–∫—Ç–æ–≤: {e}\n\n–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã:\n{facts_summary}"

        context_text = f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_found} –¥–∏–∞–ª–æ–≥–æ–≤. –ò–∑–≤–ª–µ—á–µ–Ω—ã —Ñ–∞–∫—Ç—ã.\n\n"
        context_text += f"–í—Å–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã:\n{facts_summary}\n\n"
        context_text += "–ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤:\n"
        for item, score in found_with_scores:
            context_text += f"ID: {item['id']}\n–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f}\n–¢–µ–∫—Å—Ç: {item['text'][:300]}...\n---\n"

        return final_answer, context_text

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≤ fact_extraction_analysis: {e}"
        logger.error(error_msg)
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}", f"–û—à–∏–±–∫–∞: {error_msg}"


# === –ü–æ–¥—Ö–æ–¥ 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ ===
def classification_analysis(question, found_with_scores, categories=None, status_callback=None):
    try:
        total_found = len(found_with_scores)
        if status_callback:
            status_callback(f"üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

        if not categories:
            category_prompt = (
                f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: '{question}'. "
                f"–û–ø—Ä–µ–¥–µ–ª–∏ 3-5 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Ç–∏–ø–∞ –∑–≤–æ–Ω–∫–∞), "
                f"–ø–æ –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥–∏. "
                f"–û—Ç–≤–µ—Ç—å —Å–ø–∏—Å–∫–æ–º, –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞–∑–≤–∞–Ω–∏—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —Å—Ç—Ä–æ–∫—É."
            )
            try:
                categories_response = call_ollama_with_retry(category_prompt)
                categories = [cat.strip() for cat in categories_response.strip().split('\n') if cat.strip()]
                logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {categories}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
                categories = ["–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞", "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã", "–ñ–∞–ª–æ–±–∞", "–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è"]

        if status_callback:
            status_callback(f"üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {categories}. –ù–∞—á–∏–Ω–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–∏–∞–ª–æ–≥–æ–≤...")

        classified_dialogs = {cat: [] for cat in categories}
        classified_dialogs["–ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ"] = []
        
        for i, (item, score) in enumerate(found_with_scores):
            if status_callback and i % 10 == 0:
                status_callback(f"üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i+1}/{total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

            text_snippet = item['text'][:800] + ("..." if len(item['text']) > 800 else "")
            prompt = (
                f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π –¥–∏–∞–ª–æ–≥ –ø–æ –æ–¥–Ω–æ–π –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π. "
                f"–ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –æ—Ç–≤–µ—Ç—å—Ç–µ '–ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ'.\n"
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(categories)}\n"
                f"–î–∏–∞–ª–æ–≥:\n{text_snippet}\n"
                f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è:"
            )
            
            try:
                category_response = call_ollama_with_retry(prompt)
                determined_category = category_response.strip()
                if determined_category in classified_dialogs:
                    classified_dialogs[determined_category].append((item, score))
                else:
                    classified_dialogs["–ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ"].append((item, score))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–∞ {item['id']}: {e}")
                classified_dialogs["–ù–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ"].append((item, score))

        if status_callback:
            status_callback("üß† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

        analysis_lines = []
        context_lines = [f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_found} –¥–∏–∞–ª–æ–≥–æ–≤.\n"]
        for category, dialogs in classified_dialogs.items():
            count = len(dialogs)
            analysis_lines.append(f"- {category}: {count} –¥–∏–∞–ª–æ–≥–æ–≤")
            if dialogs:
                context_lines.append(f"\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category} ({count} –¥–∏–∞–ª–æ–≥–æ–≤)")
                for item, score in dialogs[:3]:
                    context_lines.append(f"  ID: {item['id']} (–°—Ö–æ–∂–µ—Å—Ç—å: {score:.4f})")
                if len(dialogs) > 3:
                    context_lines.append(f"  ... –∏ –µ—â—ë {len(dialogs) - 3} –¥–∏–∞–ª–æ–≥–æ–≤.")

        analysis_text = "\n".join(analysis_lines)
        context_text = "\n".join(context_lines)

        final_prompt = (
            f"–í—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞. –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤. "
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏—Ö –∏ –¥–∞–π—Ç–µ –∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–∏–ø–æ–≤ –∑–≤–æ–Ω–∫–æ–≤. "
            f"–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±—ã–ª –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞ –Ω–µ–π.\n"
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n"
            f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n{analysis_text}\n"
            f"–í—ã–≤–æ–¥:"
        )
        
        try:
            final_answer = call_ollama_with_retry(final_prompt)
        except Exception as e:
            final_answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n{analysis_text}"

        return final_answer, context_text

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≤ classification_analysis: {e}"
        logger.error(error_msg)
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}", f"–û—à–∏–±–∫–∞: {error_msg}"


# === –ü–æ–¥—Ö–æ–¥ 5: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –æ–±—Ä–∞—Ç–Ω—ã–º –∑–≤–æ–Ω–∫–∞–º (–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ 1-4) ‚Äî –ß–ò–°–¢–´–ô LLM-–ê–ù–ê–õ–ò–ó ===
def callback_classifier(question, found_with_scores, chunk_size=1, status_callback=None):
    try:
        total_found = len(found_with_scores)
        if status_callback:
            status_callback(f"üìû LLM-–∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {total_found} –¥–∏–∞–ª–æ–≥–æ–≤...")

        dialog_texts = {}
        for item, score in found_with_scores:
            dialog_id = item['dialog_id']
            if dialog_id not in dialog_texts:
                dialog_texts[dialog_id] = item['full_dialog_text']

        results = []
        context_lines = []

        for dialog_id, full_text in dialog_texts.items():
            if status_callback:
                status_callback(f"üìû –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ {dialog_id}...")

            prompt = f"""–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –∫–∞—á–µ—Å—Ç–≤–∞ call-—Ü–µ–Ω—Ç—Ä–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –ø—Ä–∞–≤–∏–ª—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∑–≤–æ–Ω–∫–∞.

–ü—Ä–∞–≤–∏–ª–∞:
1. –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —Å–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç (–±–µ–∑ —Å–ª–æ–≤ "–µ—Å–ª–∏ —á—Ç–æ", "–∫–∞–∫-–Ω–∏–±—É–¥—å", "–º–æ–∂–µ—Ç –±—ã—Ç—å", "–≤–æ–∑–º–æ–∂–Ω–æ", "–Ω–µ –∑–Ω–∞—é", "—Å –¥—Ä—É–≥–æ–≥–æ –Ω–æ–º–µ—Ä–∞") ‚Äî –æ–ø–µ—Ä–∞—Ç–æ—Ä –û–ë–Ø–ó–ê–ù –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫. –ï—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª ‚Äî —ç—Ç–æ –æ—à–∏–±–∫–∞ (–ö–∞—Ç–µ–≥–æ—Ä–∏—è 1).
2. –ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫ (—Å –≤—Ä–µ–º–µ–Ω–µ–º, —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º, —É–º–µ—Å—Ç–Ω–æ) ‚Äî –ö–∞—Ç–µ–≥–æ—Ä–∏—è 2.
3. –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —Å–∫–∞–∑–∞–ª "–µ—Å–ª–∏ —á—Ç–æ", "–∫–∞–∫-–Ω–∏–±—É–¥—å –ø–æ—Ç–æ–º", "–º–æ–∂–µ—Ç –±—ã—Ç—å", "–Ω–µ –∑–Ω–∞—é" ‚Äî –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫ –ù–ï —Ç—Ä–µ–±—É–µ—Ç—Å—è. –ï—Å–ª–∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä –µ–≥–æ –Ω–∞–∑–Ω–∞—á–∏–ª ‚Äî —ç—Ç–æ –æ—à–∏–±–∫–∞ (–ö–∞—Ç–µ–≥–æ—Ä–∏—è 3).
4. –ï—Å–ª–∏ –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫ –Ω–µ —Ç—Ä–µ–±–æ–≤–∞–ª—Å—è –∏ –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω ‚Äî –≤—Å—ë –ø—Ä–∞–≤–∏–ª—å–Ω–æ (–ö–∞—Ç–µ–≥–æ—Ä–∏—è 4).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥.
- –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é (1, 2, 3, 4).
- –í—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {{"category": N, "client_phrases": ["—Ñ—Ä–∞–∑–∞1", "—Ñ—Ä–∞–∑–∞2"], "operator_phrases": ["—Ñ—Ä–∞–∑–∞1", "—Ñ—Ä–∞–∑–∞2"]}}
- –§—Ä–∞–∑—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –î–û–°–õ–û–í–ù–´–ú–ò, –∫–∞–∫ –≤ –¥–∏–∞–ª–æ–≥–µ.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, –Ω–µ –ø–∏—à–∏ "–¥—É–º–∞—é", –Ω–µ –¥–æ–±–∞–≤–ª—è–π markdown.
- –ï—Å–ª–∏ —Ñ—Ä–∞–∑ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ [].
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ—Ä–∞–∑—ã. –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ.

–ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞:
{{"category": 1, "client_phrases": ["—è —Å–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω—é –≤–µ—á–µ—Ä–æ–º"], "operator_phrases": []}}

–î–∏–∞–ª–æ–≥:
{full_text}
"""

            try:
                llm_response = call_ollama_with_retry(prompt)
                result = json.loads(llm_response)
                if "category" not in result or "client_phrases" not in result or "operator_phrases" not in result:
                    raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM")
                result["dialog_id"] = dialog_id
                results.append(result)

                cat_names = {
                    1: "‚ùå –ü—Ä–æ–ø—É—â–µ–Ω –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∑–≤–æ–Ω–æ–∫",
                    2: "‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –∑–≤–æ–Ω–æ–∫",
                    3: "‚ö†Ô∏è –ù–µ–Ω—É–∂–Ω—ã–π –∑–≤–æ–Ω–æ–∫ –Ω–∞–∑–Ω–∞—á–µ–Ω",
                    4: "‚úîÔ∏è –ó–≤–æ–Ω–æ–∫ –Ω–µ —Ç—Ä–µ–±–æ–≤–∞–ª—Å—è –∏ –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω"
                }
                context_lines.append(f"ID: {dialog_id} | {cat_names[result['category']]}")
                if result['client_phrases']:
                    context_lines.append(f"  –ö–ª–∏–µ–Ω—Ç: {', '.join(result['client_phrases'])}")
                if result['operator_phrases']:
                    context_lines.append(f"  –û–ø–µ—Ä–∞—Ç–æ—Ä: {', '.join(result['operator_phrases'])}")
                context_lines.append("---")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: {e}")
                results.append({
                    "dialog_id": dialog_id,
                    "category": 0,
                    "client_phrases": [],
                    "operator_phrases": [],
                    "error": str(e)
                })
                context_lines.append(f"ID: {dialog_id} | ‚ùå –û–®–ò–ë–ö–ê –ê–ù–ê–õ–ò–ó–ê: {e}")
                context_lines.append("---")

        final_answer = json.dumps(results, ensure_ascii=False, indent=2)
        context_text = "üìû –†–µ–∑—É–ª—å—Ç–∞—Ç—ã LLM-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞—Ç–Ω—ã–º –∑–≤–æ–Ω–∫–∞–º:\n\n" + "\n".join(context_lines)

        return final_answer, context_text

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≤ callback_classifier: {e}"
        logger.error(error_msg)
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}", f"–û—à–∏–±–∫–∞: {error_msg}"


# === –ë–´–°–¢–†–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –ù–ê –û–°–ù–û–í–ï –°–õ–û–í–ê–†–Ø ===
def fast_phrase_classifier(question, found_with_scores, chunk_size=1, status_callback=None):
    results = []
    for item, score in found_with_scores:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞ –∏–∑ –¥–∏–∞–ª–æ–≥–∞
        client_lines = []
        dialog_text = item['full_dialog_text']
        lines = dialog_text.splitlines()
        for line in lines:
            if line.startswith("–ö–ª–∏–µ–Ω—Ç:") or line.startswith("–∫–ª–∏–µ–Ω—Ç:") or line.startswith("–ö–õ–ò–ï–ù–¢:"):
                client_lines.append(line)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Ä–µ–ø–ª–∏–∫—É –∫–ª–∏–µ–Ω—Ç–∞
        category = 4  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å—ë –Ω–æ—Ä–º–∞–ª—å–Ω–æ
        for client_line in client_lines:
            # –£–±–∏—Ä–∞–µ–º "–ö–ª–∏–µ–Ω—Ç:" –∏ –ø—Ä–æ–±–µ–ª—ã
            clean_client_line = client_line.replace("–ö–ª–∏–µ–Ω—Ç:", "").replace("–∫–ª–∏–µ–Ω—Ç:", "").strip()
            if clean_client_line:
                # –ò—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ
                if any(phrase in clean_client_line for phrase in phrase_dict["category_1_phrases"]["client"]):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                    if any(excl in clean_client_line for excl in phrase_dict["category_3_phrases"]["client"]):
                        category = 3
                    elif any(excl in clean_client_line for excl in phrase_dict["category_2_phrases"]["operator"]):
                        category = 2
                    else:
                        category = 1
                    break
        
        results.append({
            "dialog_id": item['dialog_id'],
            "category": category,
            "client_phrases": [],
            "operator_phrases": []
        })
    final_answer = json.dumps(results, ensure_ascii=False, indent=2)
    context_text = "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ —Å–ª–æ–≤–∞—Ä—é."
    return final_answer, context_text


# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é ===
def get_analysis_method(method_name):
    methods = {
        "hierarchical": hierarchical_analysis,
        "rolling": rolling_summary_analysis,
        "facts": fact_extraction_analysis,
        "classification": classification_analysis,
        "callback_classifier": callback_classifier,
        "fast_phrase_classifier": fast_phrase_classifier,
    }
    return methods.get(method_name)

===== FILE: analyze_dialogs.py =====
# analyze_dialogs.py
import json
import config
from pathlib import Path
from striprtf.striprtf import rtf_to_text
import ollama # –î–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–ª–∏ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ---
# –û–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ—Ä–æ–≥–∏ –¥–ª–∏–Ω—ã –≤ —Ç–æ–∫–µ–Ω–∞—Ö
LENGTH_THRESHOLDS = [512, 1024, 2048, 4096]

def count_tokens_approx(text):
    """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤."""
    # –û—á–µ–Ω—å –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ~= 4 —Å–∏–º–≤–æ–ª–∞
    return len(text) / 4

def analyze():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Ö –¥–ª–∏–Ω–µ."""
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–∞–ø–∫–∞–º...")
    
    total_chars = 0
    total_tokens_approx = 0
    dialog_count = 0
    token_counts = []
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø–æ —Ç–µ–º–∞–º
    theme_stats = {}

    # --- –ù–û–í–û–ï: –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø–æ—Ä–æ–≥–æ–≤ ---
    threshold_counts = {thresh: 0 for thresh in LENGTH_THRESHOLDS}
    above_max_threshold = 0 # –°—á–µ—Ç—á–∏–∫ –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –ø–æ—Ä–æ–≥–∞

    for theme_folder in config.INPUT_ROOT.iterdir():
        if theme_folder.is_dir():
            theme_name = theme_folder.name
            print(f"  üìÇ –ê–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏ —Ç–µ–º—ã: {theme_name}")
            
            theme_dialog_count = 0
            theme_total_tokens = 0
            theme_token_counts = []

            for file in theme_folder.glob("*.rtf"):
                try:
                    raw_text = rtf_to_text(file.read_text(encoding="utf-8", errors="ignore"))
                    chars = len(raw_text)
                    tokens = count_tokens_approx(raw_text)
                    
                    total_chars += chars
                    total_tokens_approx += tokens
                    token_counts.append(tokens)
                    dialog_count += 1
                    
                    theme_dialog_count += 1
                    theme_total_tokens += tokens
                    theme_token_counts.append(tokens)
                    
                    # --- –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤ ---
                    for thresh in LENGTH_THRESHOLDS:
                        if tokens <= thresh:
                            threshold_counts[thresh] += 1
                    if tokens > LENGTH_THRESHOLDS[-1]: # –î–ª–∏–Ω–Ω–µ–µ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –ø–æ—Ä–æ–≥–∞
                         above_max_threshold += 1

                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file}: {e}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–µ
            if theme_dialog_count > 0:
                theme_stats[theme_name] = {
                    "count": theme_dialog_count,
                    "avg_tokens": theme_total_tokens / theme_dialog_count,
                    "max_tokens": max(theme_token_counts),
                    "min_tokens": min(theme_token_counts),
                }

    # --- –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    if dialog_count > 0:
        avg_chars = total_chars / dialog_count
        avg_tokens = total_tokens_approx / dialog_count
        max_tokens = max(token_counts)
        min_tokens = min(token_counts)
        
        print(f"\n--- –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –¥–∏–∞–ª–æ–≥–∞–º ---")
        print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {dialog_count}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {avg_chars:.2f}")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {avg_tokens:.2f}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {max_tokens:.2f}")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): {min_tokens:.2f}")
        
        if max_tokens > LENGTH_THRESHOLDS[0]:
            print(f"\n--- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º ---")
            print(f"(–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ—Ä–æ–≥–∏: {LENGTH_THRESHOLDS})")
            for thresh in LENGTH_THRESHOLDS:
                percentage = (threshold_counts[thresh] / dialog_count) * 100
                print(f"  ‚è±Ô∏è  –î–∏–∞–ª–æ–≥–æ–≤ —Å <= {thresh} —Ç–æ–∫–µ–Ω–æ–≤: {threshold_counts[thresh]} ({percentage:.2f}%)")
            if above_max_threshold > 0:
                percentage_above = (above_max_threshold / dialog_count) * 100
                print(f"  ‚è±Ô∏è  –î–∏–∞–ª–æ–≥–æ–≤ —Å > {LENGTH_THRESHOLDS[-1]} —Ç–æ–∫–µ–Ω–æ–≤: {above_max_threshold} ({percentage_above:.2f}%)")
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
            print(f"\n--- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ---")
            # –ù–∞–π–¥–µ–º –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫—Ä—ã–≤–∞–µ—Ç >= 95% –¥–∏–∞–ª–æ–≥–æ–≤
            target_coverage = 95.0
            recommended_length = None
            for thresh in sorted(LENGTH_THRESHOLDS, reverse=True):
                percentage = (threshold_counts[thresh] / dialog_count) * 100
                if percentage >= target_coverage:
                    recommended_length = thresh
            
            if recommended_length:
                print(f"  ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è MODEL.max_seq_length = {recommended_length} (–ø–æ–∫—Ä—ã–≤–∞–µ—Ç ~{target_coverage}% –¥–∏–∞–ª–æ–≥–æ–≤).")
            else:
                # –ï—Å–ª–∏ –¥–∞–∂–µ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –ø–æ—Ä–æ–≥ –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç 95%
                if above_max_threshold > 0:
                    percentage_above = (above_max_threshold / dialog_count) * 100
                    print(f"  ‚ö†Ô∏è  –ù–µ—Ç –ø–æ—Ä–æ–≥–∞, –ø–æ–∫—Ä—ã–≤–∞—é—â–µ–≥–æ {target_coverage}%. {percentage_above:.2f}% –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ {LENGTH_THRESHOLDS[-1]} —Ç–æ–∫–µ–Ω–æ–≤.")
                    print(f"  üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ MODEL.max_seq_length = {LENGTH_THRESHOLDS[-1]} –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤.")
                else:
                     # –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ —É–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                     print(f"  ‚úÖ MODEL.max_seq_length = {LENGTH_THRESHOLDS[-1]} –ø–æ–∫—Ä–æ–µ—Ç 100% –¥–∏–∞–ª–æ–≥–æ–≤.")
                     
        else:
            print(f"\n‚úÖ –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ –∫–æ—Ä–æ—á–µ {LENGTH_THRESHOLDS[0]} —Ç–æ–∫–µ–Ω–æ–≤. MODEL.max_seq_length = 512 –±—É–¥–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º.")
            
        # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º ---
        if theme_stats:
            print(f"\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º ---")
            for theme, stats in theme_stats.items():
                print(f"  üìÅ {theme}:")
                print(f"    - –§–∞–π–ª–æ–≤: {stats['count']}")
                print(f"    - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ (—Ç–æ–∫–µ–Ω—ã): {stats['avg_tokens']:.2f}")
                print(f"    - –ú–∏–Ω/–ú–∞–∫—Å –¥–ª–∏–Ω–∞: {stats['min_tokens']:.2f} / {stats['max_tokens']:.2f}")

    else:
        print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ .rtf —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

if __name__ == "__main__":
    analyze()

===== FILE: classifier.py =====
# classifier.py ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ —Å–ª–æ–≤–∞—Ä—é —Ñ—Ä–∞–∑

import json
import os

# –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é —Ñ—Ä–∞–∑
PHRASE_DICT_PATH = "data/aggregated_phrases.json"

def load_phrase_dict():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ñ—Ä–∞–∑ –∏–∑ JSON-—Ñ–∞–π–ª–∞."""
    if not os.path.exists(PHRASE_DICT_PATH):
        raise FileNotFoundError(f"–°–ª–æ–≤–∞—Ä—å —Ñ—Ä–∞–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω: {PHRASE_DICT_PATH}")
    
    with open(PHRASE_DICT_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

def classify_dialog_with_phrases(dialog_text, phrase_dict=None):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ –æ–¥–Ω—É –∏–∑ 4 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä—è —Ñ—Ä–∞–∑.
    
    Args:
        dialog_text (str): –¢–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞.
        phrase_dict (dict): –°–ª–æ–≤–∞—Ä—å —Ñ—Ä–∞–∑ (–µ—Å–ª–∏ None ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞).
    
    Returns:
        int: –ö–∞—Ç–µ–≥–æ—Ä–∏—è (1, 2, 3, 4).
    """
    if phrase_dict is None:
        phrase_dict = load_phrase_dict()
    
    # –ò—â–µ–º —Ñ—Ä–∞–∑—ã –∫–ª–∏–µ–Ω—Ç–∞ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 1
    for phrase in phrase_dict["category_1_phrases"]["client"]:
        if phrase in dialog_text:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è: —Ñ—Ä–∞–∑—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 2 (–æ–ø–µ—Ä–∞—Ç–æ—Ä) –∏ 3 (–∫–ª–∏–µ–Ω—Ç)
            for excl_phrase in phrase_dict["category_3_phrases"]["client"]:
                if excl_phrase in dialog_text:
                    return 3  # –ö–ª–∏–µ–Ω—Ç –¥–∞–ª –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å ‚Üí –∑–≤–æ–Ω–æ–∫ –Ω–µ –Ω—É–∂–µ–Ω, –Ω–æ –Ω–∞–∑–Ω–∞—á–µ–Ω
            
            for excl_phrase in phrase_dict["category_2_phrases"]["operator"]:
                if excl_phrase in dialog_text:
                    return 2  # –û–ø–µ—Ä–∞—Ç–æ—Ä –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∑–≤–æ–Ω–æ–∫
            
            return 1  # –û—à–∏–±–∫–∞: –∫–ª–∏–µ–Ω—Ç –æ–±–µ—â–∞–ª –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å ‚Üí –æ–ø–µ—Ä–∞—Ç–æ—Ä –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª
    
    return 4  # –ó–≤–æ–Ω–æ–∫ –Ω–µ —Ç—Ä–µ–±–æ–≤–∞–ª—Å—è –∏ –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω

===== FILE: config.py =====
"""–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ CallCenter AI v3 (—Ä–µ–ø–ª–∏–∫–∏ + HyDE + Reranker)."""

import os
from pathlib import Path

# --- –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏ ---
PROJECT_ROOT = Path(__file__).parent.resolve()
INPUT_ROOT = PROJECT_ROOT / "Input"
PROCESSED_ROOT = PROJECT_ROOT / "processed"
LOGS_ROOT = PROJECT_ROOT / "logs"
FAISS_INDEX_DIR = PROJECT_ROOT / "faiss_index"      # –ò–Ω–¥–µ–∫—Å—ã FAISS
CACHE_ROOT = PROJECT_ROOT / "cache"                 # –ö—ç—à (–Ω–∞–ø—Ä–∏–º–µ—Ä, search_cache)
EXPORTS_ROOT = PROJECT_ROOT / "exports"             # –≠–∫—Å–ø–æ—Ä—Ç—ã –∏–∑ GUI
TEMP_ROOT = PROJECT_ROOT / "temp"                   # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

# –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–∞–ø–æ–∫
for path in [INPUT_ROOT, PROCESSED_ROOT, LOGS_ROOT, FAISS_INDEX_DIR, CACHE_ROOT, EXPORTS_ROOT, TEMP_ROOT]:
    os.makedirs(path, exist_ok=True)

# --- –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö ---
DATABASE_PATH = PROJECT_ROOT / "database.db"

# --- –ú–æ–¥–µ–ª–∏ ---
## –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–¥–ª—è —Ä–µ–ø–ª–∏–∫ –∏ –¥–∏–∞–ª–æ–≥–æ–≤)
EMBEDDING_MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"
EMBEDDING_MODEL_DEVICE = "cuda"          # "cuda" –∏–ª–∏ "cpu"
EMBEDDING_MODEL_PRECISION = "float16"    # "float16" –¥–ª—è GPU, "float32" –¥–ª—è CPU
EMBEDDING_MODEL_MAX_LENGTH = 4096
EMBEDDING_MODEL_DIMENSION = 1024

## Reranker (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
RERANKER_MODEL_NAME = "BAAI/bge-reranker-large"
RERANKER_MAX_LENGTH = 512
RERANKER_ENABLED = True

## –ú–æ–¥–µ–ª—å –¥–ª—è LLM (Ollama)
LLM_MODEL_NAME = "dimweb/ilyagusev-saiga_llama3_8b:kto_v5_Q4_K"
LLM_API_URL = "http://localhost:11434/api/generate"  # URL Ollama API

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---
PIPELINE_BATCH_SIZE = 32               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–æ–≤
PIPELINE_LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
PIPELINE_LOG_JSON_FORMAT = False         # True –¥–ª—è JSON-–ª–æ–≥–æ–≤ (ELK, Grafana)

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ FAISS ---
FAISS_INDEX_TYPE = "IndexFlatIP"         # –¢–∏–ø –∏–Ω–¥–µ–∫—Å–∞ FAISS
FAISS_NLIST = 100                        # –î–ª—è IVF –∏–Ω–¥–µ–∫—Å–æ–≤ (–µ—Å–ª–∏ –±—É–¥–µ—à—å –º–µ–Ω—è—Ç—å —Ç–∏–ø)
FAISS_M = 32                             # –î–ª—è HNSW (–µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è)

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∏ GUI ---
GUI_DEFAULT_TOP_K = 5                    # –°–∫–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å
GUI_DEFAULT_CHUNK_SIZE = 10              # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è analysis_methods
GUI_DEFAULT_METHOD = "hierarchical"
ANALYSIS_METHODS = ["hierarchical", "rolling", "facts", "classification", "callback_classifier", "fast_phrase_classifier"]

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ HyDE ---
HYDE_ENABLED = True
HYDE_PROMPT_TEMPLATE = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ call-—Ü–µ–Ω—Ç—Ä–∞.
–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Ç–æ—á–Ω—ã–π –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã —É–∂–µ –Ω–∞—à—ë–ª –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –¥–∏–∞–ª–æ–≥–æ–≤.

–í–æ–ø—Ä–æ—Å: {query}

–ì–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç:"""

# --- –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ GUI (–¢–µ–º–Ω–∞—è —Ç–µ–º–∞) ---
GUI_THEME = {
    "dark_bg": '#2e2e2e',
    "dark_fg": '#ffffff',
    "dark_entry_bg": '#4d4d4d',
    "dark_button_bg": '#5a5a5a',
    "dark_frame_bg": '#3d3d3d',
    "success_color": '#4CAF50',   # –ó–µ–ª–µ–Ω—ã–π
    "warning_color": '#FF9800',   # –û—Ä–∞–Ω–∂–µ–≤—ã–π
    "error_color": '#F44336',     # –ö—Ä–∞—Å–Ω—ã–π
    "info_color": '#2196F3',      # –°–∏–Ω–∏–π
    "highlight_color": '#9C27B0'  # –§–∏–æ–ª–µ—Ç–æ–≤—ã–π
}

# --- –°–∏—Å—Ç–µ–º–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---
MAX_WORKERS = 4                          # –î–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ GUI
LLM_TIMEOUT = 60                         # –¢–∞–π–º–∞—É—Ç –¥–ª—è LLM –∑–∞–ø—Ä–æ—Å–æ–≤ (—Å–µ–∫)
DEBUG_MODE = False                       # –†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ (–±–æ–ª—å—à–µ –ª–æ–≥–æ–≤)

# config.py (–≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞)

# === –í—Å–µ 55 FREE –º–æ–¥–µ–ª–µ–π OpenRouter ===
OPENROUTER_FREE_MODELS = [
    "deepseek/deepseek-chat-v3.1:free",
    "deepseek/deepseek-chat-v3-0324:free",
    "deepseek/deepseek-r1-0528:free",
    "deepseek/deepseek-r1:free",
    "tngtech/deepseek-r1t2-chimera:free",
    "qwen/qwen3-coder:free",
    "z-ai/glm-4.5-air:free",
    "tngtech/deepseek-r1t-chimera:free",
    "moonshotai/kimi-k2:free",
    "qwen/qwen3-235b-a22b:free",
    "meta-llama/llama-3.3-70b-instruct:free",
    "google/gemini-2.0-flash-exp:free",
    "microsoft/mai-ds-r1:free",
    "qwen/qwen2.5-vl-72b-instruct:free",
    "openai/gpt-oss-20b:free",
    "mistralai/mistral-small-3.2-24b-instruct:free",
    "meta-llama/llama-4-maverick:free",
    "cognitivecomputations/dolphin-mistral-24b-venice-edition:free",
    "qwen/qwen3-14b:free",
    "deepseek/deepseek-r1-distill-llama-70b:free",
    "deepseek/deepseek-r1-0528-qwen3-8b:free",
    "nvidia/nemotron-nano-9b-v2:free",
    "mistralai/mistral-nemo:free",
    "meta-llama/llama-3.1-405b-instruct:free",
    "qwen/qwen-2.5-coder-32b-instruct:free",
    "google/gemma-3-27b-it:free",
    "meta-llama/llama-3.3-8b-instruct:free",
    "agentica-org/deepcoder-14b-preview:free",
    "moonshotai/kimi-dev-72b:free",
    "qwen/qwen3-30b-a3b:free",
    "mistralai/mistral-7b-instruct:free",
    "qwen/qwen-2.5-72b-instruct:free",
    "meta-llama/llama-4-scout:free",
    "mistralai/mistral-small-3.1-24b-instruct:free",
    "qwen/qwq-32b:free",
    "qwen/qwen3-8b:free",
    "mistralai/devstral-small-2505:free",
    "qwen/qwen3-4b:free",
    "qwen/qwen2.5-vl-32b-instruct:free",
    "shisa-ai/shisa-v2-llama3.3-70b:free",
    "cognitivecomputations/dolphin3.0-mistral-24b:free",
    "moonshotai/kimi-vl-a3b-thinking:free",
    "nousresearch/deephermes-3-llama-3-8b-preview:free",
    "meta-llama/llama-3.2-3b-instruct:free",
    "tencent/hunyuan-a13b-instruct:free",
    "google/gemma-3-12b-it:free",
    "arliai/qwq-32b-arliai-rpr-v1:free",
    "mistralai/mistral-small-24b-instruct-2501:free",
    "google/gemma-3n-e2b-it:free",
    "google/gemma-2-9b-it:free",
    "openai/gpt-oss-120b:free",
    "cognitivecomputations/dolphin3.0-r1-mistral-24b:free",
    "google/gemma-3n-e4b-it:free",
    "google/gemma-3-4b-it:free",
    "rekaai/reka-flash-3:free"
]

# –¢–≤–æ–π –∫–ª—é—á OpenRouter
OPENROUTER_API_KEY = "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"

# config.py (–¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü)

# === –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–ª–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===
CLOUD_DATABASE_PATH = PROJECT_ROOT / "callback_cloud.db"

# config.py (–¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü)

# === –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π LLM API (–ë–∏–ª–∞–π–Ω) ===
LLM_INTERNAL_API_KEY = "Qs9H8I1q4M2XEo23h11anlob2OSHBqj-h4fsLY4jnrY"
LLM_INTERNAL_API_URL = "https://ta.apps.yd-m5-k51.vimpelcom.ru/api/v1/chat/completions"
LLM_INTERNAL_MODEL = "Qwen3-32B" # –ò–ª–∏ "RuadaptQwen" –∏–ª–∏ "QwQ"

===== FILE: export_phrases.py =====
import sqlite3
import json

conn = sqlite3.connect("database.db")
cursor = conn.cursor()

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã
cursor.execute("SELECT phrase, source, category FROM callback_phrases ORDER BY category, phrase")
rows = cursor.fetchall()

# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
aggregated = {
    "category_1_phrases": {"client": [], "operator": []},
    "category_2_phrases": {"client": [], "operator": []},
    "category_3_phrases": {"client": [], "operator": []},
    "category_4_phrases": {"client": [], "operator": []}
}

for phrase, source, category in rows:
    key = f"category_{category}_phrases"
    if source == "client_promise":
        aggregated[key]["client"].append(phrase)
    elif source == "operator_offer":
        aggregated[key]["operator"].append(phrase)
    elif source == "client_uncertain":
        aggregated[key]["client"].append(phrase)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
with open("data/aggregated_phrases.json", "w", encoding="utf-8") as f:
    json.dump(aggregated, f, ensure_ascii=False, indent=2)

print("‚úÖ –°–ª–æ–≤–∞—Ä—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: data/aggregated_phrases.json")
conn.close()

===== FILE: generate_callback_phrases.py =====
# generate_callback_phrases.py
import sqlite3
import json
import logging
import time
import requests
import re
from typing import Optional, Dict, Any

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("llm_callback_errors.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
DB_PATH = "database.db"
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# –ú–ò–ù–ò–ú–ê–õ–ò–°–¢–ò–ß–ù–´–ô –ü–†–û–ú–ü–¢ ‚Äî —Ç–æ–ª—å–∫–æ —Å—É—Ç—å, –±–µ–∑ –≤–æ–¥—ã
PROMPT_TEMPLATE = """–¢—ã ‚Äî JSON-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π. –§–æ—Ä–º–∞—Ç: {"error": true/false, "client_phrase": "...", "operator_phrase": "..."}

–ü—Ä–∞–≤–∏–ª–æ:
- error = true: –∫–ª–∏–µ–Ω—Ç –ß–Å–¢–ö–û —Å–∫–∞–∑–∞–ª "–ø–µ—Ä–µ–∑–≤–æ–Ω—é/—Å–≤—è–∂—É—Å—å/–Ω–∞–±–µ—Ä—É" –ë–ï–ó "–µ—Å–ª–∏/–º–æ–∂–µ—Ç/–∫–∞–∫-–Ω–∏–±—É–¥—å" ‚Üí –∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä –ù–ï –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∑–≤–æ–Ω–æ–∫.
- error = false: –∫–ª–∏–µ–Ω—Ç –¥–∞–ª –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –ò–õ–ò –æ–ø–µ—Ä–∞—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∑–≤–æ–Ω–æ–∫.

–§—Ä–∞–∑—ã ‚Äî –¥–æ—Å–ª–æ–≤–Ω–æ –∏–∑ –¥–∏–∞–ª–æ–≥–∞. –ù–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ.

–î–∏–∞–ª–æ–≥:
{dialog_text}
"""


def escape_curly_brackets(text: str) -> str:
    return text.replace("{", "{{").replace("}", "}}")


def extract_first_json(text: str) -> Optional[str]:
    start = text.find('{')
    if start == -1:
        return None
    for end in range(start + 1, len(text) + 1):
        try:
            candidate = text[start:end]
            json.loads(candidate)
            return candidate
        except json.JSONDecodeError:
            continue
    return None


def call_llm_with_retry(dialog_text: str, max_retries=2) -> Optional[Dict[str, Any]]:  # ‚Üê –£–º–µ–Ω—å—à–∏–ª –¥–æ 2 –ø–æ–ø—ã—Ç–æ–∫
    for attempt in range(max_retries):
        try:
            safe_dialog_text = escape_curly_brackets(dialog_text)
            prompt = PROMPT_TEMPLATE.format(dialog_text=safe_dialog_text)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ api/generate ‚Äî –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ ‚Üí –±—ã—Å—Ç—Ä–µ–µ
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    "model": "qwen2.5:1.5b-instruct-q4_K_M",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 128,  # ‚Üê –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
                        "temperature": 0.1,  # ‚Üê –î–µ–ª–∞–µ–º –æ—Ç–≤–µ—Ç –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
                        "top_p": 0.9
                    }
                },
                timeout=30  # ‚Üê –£–º–µ–Ω—å—à–∞–µ–º —Ç–∞–π–º–∞—É—Ç
            )

            if response.status_code != 200:
                logging.warning(f"HTTP {response.status_code}: {response.text}")
                raise Exception(f"HTTP Error {response.status_code}")

            ollama_json = response.json()

            if "error" in ollama_json:
                error_msg = ollama_json["error"]
                logging.warning(f"Ollama error: {error_msg}")
                raise Exception(f"Ollama error: {error_msg}")

            raw_text = ollama_json.get("response", "").strip()
            logging.info(f"LLM raw response: {repr(raw_text)}")

            json_str = extract_first_json(raw_text)
            if not json_str:
                raise ValueError("No valid JSON found in response")

            result = json.loads(json_str)
            if "error" in result and "client_phrase" in result and "operator_phrase" in result:
                return result
            else:
                raise ValueError("Missing required keys in LLM JSON")

        except Exception as e:
            logging.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
            if attempt < max_retries - 1:
                time.sleep(0.5)  # ‚Üê –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
            else:
                logging.error(f"LLM –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞ –≤–∞–ª–∏–¥–Ω–æ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞: {dialog_text[:100]}...")
                return None

    return None


# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∏–∞–ª–æ–≥–∏
cursor.execute("SELECT id, text FROM dialogs")
dialogs = cursor.fetchall()

for i, (dialog_id, dialog_text) in enumerate(dialogs, 1):
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–∞ {i}/{len(dialogs)} (ID: {dialog_id})...")

    result = call_llm_with_retry(dialog_text)
    if not result:
        continue

    error = result["error"]
    client_phrase = result["client_phrase"].strip() if result["client_phrase"] else ""
    operator_phrase = result["operator_phrase"].strip() if result["operator_phrase"] else ""

    if error and client_phrase:
        cursor.execute("""
            INSERT OR IGNORE INTO callback_phrases (phrase, source, category, verified)
            VALUES (?, ?, ?, 0)
        """, (client_phrase, 'client_promise', 1))

    if not error:
        if "–µ—Å–ª–∏" in client_phrase or "–º–æ–∂–µ—Ç" in client_phrase or "–∫–∞–∫-–Ω–∏–±—É–¥—å" in client_phrase:
            cursor.execute("""
                INSERT OR IGNORE INTO callback_phrases (phrase, source, category, verified)
                VALUES (?, ?, ?, 0)
            """, (client_phrase, 'client_uncertain', 2))
        elif operator_phrase:
            cursor.execute("""
                INSERT OR IGNORE INTO callback_phrases (phrase, source, category, verified)
                VALUES (?, ?, ?, 0)
            """, (operator_phrase, 'operator_offer', 3))

    conn.commit()

conn.close()
print("‚úÖ –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –§—Ä–∞–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É 'callback_phrases'.")
print("üìÑ –ü–æ–¥—Ä–æ–±–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ —Å—ã—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã ‚Äî –≤ —Ñ–∞–π–ª–µ llm_callback_errors.log")

===== FILE: generate_callback_phrases_cloud_db.py =====
# generate_callback_phrases_cloud_db.py
"""–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ OpenRouter API (DeepSeek V3.1). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ë–î."""

import sqlite3
import json
import requests
import time
import os
from pathlib import Path

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENROUTER_API_KEY = "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"
MODEL = "deepseek/deepseek-chat-v3.1:free"  # DeepSeek V3.1 –≤–º–µ—Å—Ç–æ OpenAI
REQUESTS_PER_MINUTE = 20  # –õ–∏–º–∏—Ç OpenRouter
DELAY_BETWEEN_REQUESTS = 60.0 / REQUESTS_PER_MINUTE  # ~3 —Å–µ–∫—É–Ω–¥—ã

# –ü—É—Ç–∏ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö
BASE_DIR = Path(__file__).parent
DATABASE_PATH = BASE_DIR / "db" / "call_center.db"  # –û—Å–Ω–æ–≤–Ω–∞—è –ë–î —Å –¥–∏–∞–ª–æ–≥–∞–º–∏
OPENROUTER_DB_PATH = BASE_DIR / "db" / "callback_phrases_openrouter.db"  # –ù–æ–≤–∞—è –ë–î –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

# –ü—Ä–æ–º–ø—Ç (–±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è .format())
PROMPT_TEMPLATE = """–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –∫–∞—á–µ—Å—Ç–≤–∞ call-—Ü–µ–Ω—Ç—Ä–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω **–æ—à–∏–±–∫—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ –ø—Ä–∞–≤–∏–ª—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∑–≤–æ–Ω–∫–∞**.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—à–∏–±–∫–∞ –µ—Å—Ç—å, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å/—Å–≤—è–∑–∞—Ç—å—Å—è/–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è ‚Äî **—á—ë—Ç–∫–æ –∏ –±–µ–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏** (–±–µ–∑ —Å–ª–æ–≤ "–µ—Å–ª–∏ —á—Ç–æ", "–º–æ–∂–µ—Ç –±—ã—Ç—å", "–∫–∞–∫-–Ω–∏–±—É–¥—å", "–Ω–µ –∑–Ω–∞—é", "—Å –¥—Ä—É–≥–æ–≥–æ –Ω–æ–º–µ—Ä–∞").
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ù–ï** –ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫.

2. –û—à–∏–±–∫–∞ –ù–ï–¢, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –¥–∞–ª **–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±–µ—â–∞–Ω–∏–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–µ—Å–ª–∏ —á—Ç–æ –ø–µ—Ä–µ–∑–≤–æ–Ω—é").
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫** (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–º—ã –≤–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–º", "–Ω–∞–∑–Ω–∞—á–∏–ª –∑–≤–æ–Ω–æ–∫ –Ω–∞ 15:00").

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥.
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –µ—Å—Ç—å ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": true, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞", "operator_phrase": ""}
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∏ –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": false, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)", "operator_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)"}

–í–ê–ñ–ù–û:
- –§—Ä–∞–∑—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –î–û–°–õ–û–í–ù–´–ú–ò, –∫–∞–∫ –≤ –¥–∏–∞–ª–æ–≥–µ.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, –Ω–µ –ø–∏—à–∏ "–¥—É–º–∞—é", –Ω–µ –¥–æ–±–∞–≤–ª—è–π markdown.
- –ï—Å–ª–∏ —Ñ—Ä–∞–∑ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É "".
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ—Ä–∞–∑—ã. –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ.

–î–∏–∞–ª–æ–≥:
"""

def ensure_db_directories():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ë–î, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç."""
    os.makedirs(os.path.dirname(DATABASE_PATH), exist_ok=True)
    os.makedirs(os.path.dirname(OPENROUTER_DB_PATH), exist_ok=True)

def find_dialogs_table(cursor):
    """–ò—â–µ—Ç —Ç–∞–±–ª–∏—Ü—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cursor.fetchall()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Ç–∞–±–ª–∏—Ü—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–µ–π id –∏ text
    for table in tables:
        table_name = table[0]
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = cursor.fetchall()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤
        has_id = False
        has_text = False
        for column in columns:
            col_name = column[1].lower()
            if "id" in col_name:
                has_id = True
            if "text" in col_name or "content" in col_name:
                has_text = True
        
        if has_id and has_text:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∏–∞–ª–æ–≥–∞–º–∏: {table_name}")
            return table_name
    
    print("‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ —Å –¥–∏–∞–ª–æ–≥–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
    return None

def extract_phrases_with_openrouter(dialog_text):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ OpenRouter API —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏."""
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –ë–ï–ó –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è .format() - –Ω–∞–ø—Ä—è–º—É—é –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
    payload = {
        "model": MODEL,
        "messages": [
            {"role": "user", "content": PROMPT_TEMPLATE + dialog_text}
        ],
        "temperature": 0.1,
        "max_tokens": 2048
    }

    for attempt in range(3):  # 3 –ø–æ–ø—ã—Ç–∫–∏
        try:
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )

            if response.status_code == 200:
                result_text = response.json()["choices"][0]["message"]["content"]
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞
                try:
                    result = json.loads(result_text)
                except json.JSONDecodeError:
                    # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ —á–∏—Å—Ç—ã–π JSON, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –µ–≥–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    if '{' in result_text and '}' in result_text:
                        json_start = result_text.find('{')
                        json_end = result_text.rfind('}') + 1
                        json_str = result_text[json_start:json_end]
                        result = json.loads(json_str)
                    else:
                        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç: {result_text}")
                        result = {"error": False, "client_phrase": "", "operator_phrase": ""}
                return result
                
            elif response.status_code == 404:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ 404: API endpoint –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL.")
                return {"error": False, "client_phrase": "", "operator_phrase": ""}
            elif response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                print(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç. –ñ–¥–µ–º {retry_after} —Å–µ–∫. (–ó–∞–≥–æ–ª–æ–≤–æ–∫ Retry-After)")
                time.sleep(retry_after)
                continue
            else:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ HTTP {response.status_code}: {response.text}")
                time.sleep(DELAY_BETWEEN_REQUESTS * 2)
                continue
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ API: {e}")
            time.sleep(DELAY_BETWEEN_REQUESTS)
            continue

    return {"error": False, "client_phrase": "", "operator_phrase": "", "error_msg": "–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å"}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤."""
    ensure_db_directories()
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –û–°–ù–û–í–ù–û–ô –ë–î –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤
    conn_main = sqlite3.connect(DATABASE_PATH)
    cursor_main = conn_main.cursor()
    
    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏
    dialogs_table = find_dialogs_table(cursor_main)
    if not dialogs_table:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏. –í—ã—Ö–æ–¥.")
        return
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –û–¢–î–ï–õ–¨–ù–û–ô –ë–î –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ OpenRouter
    conn_openrouter = sqlite3.connect(OPENROUTER_DB_PATH)
    cursor_openrouter = conn_openrouter.cursor()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ –Ω–æ–≤–æ–π –ë–î, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    cursor_openrouter.execute("""
        CREATE TABLE IF NOT EXISTS callback_phrases (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            dialog_id TEXT NOT NULL,
            phrase TEXT NOT NULL,
            source TEXT NOT NULL,
            category INTEGER NOT NULL,
            frequency INTEGER DEFAULT 1,
            verified BOOLEAN DEFAULT 0,
            raw_response TEXT,
            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∏–∞–ª–æ–≥–∏ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î
    cursor_main.execute(f"SELECT id, text FROM {dialogs_table}")
    dialogs = cursor_main.fetchall()
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ë–î.")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –ª–∏–º–∏—Ç–æ–≤
    processed_count = 0
    for i, (dialog_id, text) in enumerate(dialogs, 1):
        start_time = time.time()
        
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–∞ {i}/{len(dialogs)} (ID: {dialog_id})...")
        result = extract_phrases_with_openrouter(text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î OpenRouter
        if result.get("client_phrase") or result.get("operator_phrase"):
            raw_response_str = json.dumps(result, ensure_ascii=False)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –∏—Å—Ç–æ—á–Ω–∏–∫
            if result.get("error", False):
                source = "client_promise"
                category = 1  # –û—à–∏–±–∫–∞
            else:
                source = "client_uncertain"
                category = 3  # –ù–µ–Ω—É–∂–Ω—ã–π –∑–≤–æ–Ω–æ–∫
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—Ä–∞–∑—É –∫–ª–∏–µ–Ω—Ç–∞
            if result.get("client_phrase"):
                cursor_openrouter.execute("""
                    INSERT INTO callback_phrases (dialog_id, phrase, source, category, raw_response)
                    VALUES (?, ?, ?, ?, ?)
                """, (dialog_id, result["client_phrase"], source, category, raw_response_str))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—Ä–∞–∑—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if result.get("operator_phrase"):
                cursor_openrouter.execute("""
                    INSERT INTO callback_phrases (dialog_id, phrase, source, category, raw_response)
                    VALUES (?, ?, 'operator_offer', 2, ?)
                """, (dialog_id, result["operator_phrase"], raw_response_str))
                
            conn_openrouter.commit()
            processed_count += 1
            print(f"‚úÖ –î–∏–∞–ª–æ–≥ {dialog_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
        else:
            print(f"‚ÑπÔ∏è –î–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ—Ä–∞–∑.")
        
        # –í—ã–¥–µ—Ä–∂–∏–≤–∞–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        processing_time = time.time() - start_time
        if processing_time < DELAY_BETWEEN_REQUESTS:
            sleep_time = DELAY_BETWEEN_REQUESTS - processing_time
            print(f"‚è≥ –û–∂–∏–¥–∞–µ–º {sleep_time:.2f} —Å–µ–∫. –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞...")
            time.sleep(sleep_time)
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    conn_main.close()
    conn_openrouter.close()
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –¥–∏–∞–ª–æ–≥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OPENROUTER_DB_PATH}")

if __name__ == "__main__":
    main()

===== FILE: generate_callback_phrases_internal_llm.py =====
# generate_callback_phrases_internal_llm.py
import sqlite3
import json
import config
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

PROMPT_TEMPLATE = """–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –∫–∞—á–µ—Å—Ç–≤–∞ call-—Ü–µ–Ω—Ç—Ä–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω **–æ—à–∏–±–∫—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ –ø—Ä–∞–≤–∏–ª—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∑–≤–æ–Ω–∫–∞**.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—à–∏–±–∫–∞ –µ—Å—Ç—å, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å/—Å–≤—è–∑–∞—Ç—å—Å—è/–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è ‚Äî **—á—ë—Ç–∫–æ –∏ –±–µ–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏** (–±–µ–∑ —Å–ª–æ–≤ ‚Äú–µ—Å–ª–∏ —á—Ç–æ‚Äù, ‚Äú–º–æ–∂–µ—Ç –±—ã—Ç—å‚Äù, ‚Äú–∫–∞–∫-–Ω–∏–±—É–¥—å‚Äù, ‚Äú–Ω–µ –∑–Ω–∞—é‚Äù, ‚Äú—Å –¥—Ä—É–≥–æ–≥–æ –Ω–æ–º–µ—Ä–∞‚Äù).
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ù–ï** –ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫.

2. –û—à–∏–±–∫–∞ –ù–ï–¢, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –¥–∞–ª **–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±–µ—â–∞–Ω–∏–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú–µ—Å–ª–∏ —á—Ç–æ –ø–µ—Ä–µ–∑–≤–æ–Ω—é‚Äù).
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫** (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú–º—ã –≤–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–º‚Äù, ‚Äú–Ω–∞–∑–Ω–∞—á–∏–ª –∑–≤–æ–Ω–æ–∫ –Ω–∞ 15:00‚Äù).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥.
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –µ—Å—Ç—å ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": true, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞", "operator_phrase": ""}
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∏ –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": false, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)", "operator_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)"}

–í–ê–ñ–ù–û:
- –§—Ä–∞–∑—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –î–û–°–õ–û–í–ù–´–ú–ò, –∫–∞–∫ –≤ –¥–∏–∞–ª–æ–≥–µ.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, –Ω–µ –ø–∏—à–∏ "–¥—É–º–∞—é", –Ω–µ –¥–æ–±–∞–≤–ª—è–π markdown.
- –ï—Å–ª–∏ —Ñ—Ä–∞–∑ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É "".
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ—Ä–∞–∑—ã. –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ.

–î–∏–∞–ª–æ–≥:
"""

def safe_request(text):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –≤ —Ç–µ–ª–µ, –º–∏–Ω—É—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏"""
    headers = {
        "Authorization": f"Bearer {config.LLM_INTERNAL_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤—Ä—É—á–Ω—É—é
    full_prompt = PROMPT_TEMPLATE + text
    
    payload = {
        "model": config.LLM_INTERNAL_MODEL,
        "stream": False,
        "messages": [
            {"role": "user", "content": full_prompt}
        ]
    }

    print(f"\n[DEBUG] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º:\n{full_prompt[:500]}...\n")

    try:
        response = requests.post(
            config.LLM_INTERNAL_API_URL,
            headers=headers,
            json=payload,
            timeout=120
        )
        
        print(f"[DEBUG] –°—Ç–∞—Ç—É—Å: {response.status_code}")
        print(f"[DEBUG] –ó–∞–≥–æ–ª–æ–≤–∫–∏: {response.headers}")
        
        if response.status_code == 200:
            result_data = response.json()
            print(f"[DEBUG] –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç:\n{json.dumps(result_data, indent=2)}")
            
            result_text = result_data["choices"][0]["message"]["content"]
            print(f"[DEBUG] Raw response:\n{result_text}")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∫–ª—é—á–∞–º
            try:
                result = json.loads(result_text)
                return result
            except:
                print(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON")
                return {"error": False, "client_phrase": "", "operator_phrase": ""}
        else:
            print(f"[ERROR] –û—à–∏–±–∫–∞ API: {response.text}")
            return {"error": False, "client_phrase": "", "operator_phrase": ""}
            
    except Exception as e:
        print(f"[ERROR] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {str(e)}")
        return {"error": False, "client_phrase": "", "operator_phrase": ""}

# ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

===== FILE: generate_callback_phrases_multi_model.py =====
# generate_callback_phrases_multi_model.py
"""–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —á–µ—Ä–µ–∑ 55 FREE –º–æ–¥–µ–ª–µ–π OpenRouter –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""

import sqlite3
import json
import config
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

# –ü—Ä–æ–º–ø—Ç (—Ç–æ—Ç –∂–µ, —á—Ç–æ –∏ —Ä–∞–Ω—å—à–µ)
PROMPT_TEMPLATE = """–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä –∫–∞—á–µ—Å—Ç–≤–∞ call-—Ü–µ–Ω—Ç—Ä–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω **–æ—à–∏–±–∫—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ –ø—Ä–∞–≤–∏–ª—É –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –∑–≤–æ–Ω–∫–∞**.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—à–∏–±–∫–∞ –µ—Å—Ç—å, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç –æ –Ω–∞–º–µ—Ä–µ–Ω–∏–∏ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å/—Å–≤—è–∑–∞—Ç—å—Å—è/–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è ‚Äî **—á—ë—Ç–∫–æ –∏ –±–µ–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏** (–±–µ–∑ —Å–ª–æ–≤ ‚Äú–µ—Å–ª–∏ —á—Ç–æ‚Äù, ‚Äú–º–æ–∂–µ—Ç –±—ã—Ç—å‚Äù, ‚Äú–∫–∞–∫-–Ω–∏–±—É–¥—å‚Äù, ‚Äú–Ω–µ –∑–Ω–∞—é‚Äù, ‚Äú—Å –¥—Ä—É–≥–æ–≥–æ –Ω–æ–º–µ—Ä–∞‚Äù).
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ù–ï** –ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫.

2. –û—à–∏–±–∫–∞ –ù–ï–¢, –µ—Å–ª–∏:
   - –ö–ª–∏–µ–Ω—Ç –¥–∞–ª **–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±–µ—â–∞–Ω–∏–µ** (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú–µ—Å–ª–∏ —á—Ç–æ –ø–µ—Ä–µ–∑–≤–æ–Ω—é‚Äù).
   - –û–ø–µ—Ä–∞—Ç–æ—Ä **–ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ–±—Ä–∞—Ç–Ω—ã–π –∑–≤–æ–Ω–æ–∫** (–Ω–∞–ø—Ä–∏–º–µ—Ä, ‚Äú–º—ã –≤–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–º‚Äù, ‚Äú–Ω–∞–∑–Ω–∞—á–∏–ª –∑–≤–æ–Ω–æ–∫ –Ω–∞ 15:00‚Äù).

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥.
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –µ—Å—Ç—å ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": true, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞", "operator_phrase": ""}
- –ï—Å–ª–∏ –æ—à–∏–±–∫–∏ –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {"error": false, "client_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∫–ª–∏–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)", "operator_phrase": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ñ—Ä–∞–∑–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –±—ã–ª–∞)"}

–í–ê–ñ–ù–û:
- –§—Ä–∞–∑—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –î–û–°–õ–û–í–ù–´–ú–ò, –∫–∞–∫ –≤ –¥–∏–∞–ª–æ–≥–µ.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–π, –Ω–µ –ø–∏—à–∏ "–¥—É–º–∞—é", –Ω–µ –¥–æ–±–∞–≤–ª—è–π markdown.
- –ï—Å–ª–∏ —Ñ—Ä–∞–∑ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É "".
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ—Ä–∞–∑—ã. –¢–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ.

–î–∏–∞–ª–æ–≥:
{dialog_text}
"""

def extract_phrases_with_llm(dialog_text):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∏–∞–ª–æ–≥ –≤ —Å–ª—É—á–∞–π–Ω—É—é FREE –º–æ–¥–µ–ª—å OpenRouter —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ."""
    headers = {
        "Authorization": f"Bearer {config.OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –ø–æ –æ—á–µ—Ä–µ–¥–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    for model in config.OPENROUTER_FREE_MODELS:
        for attempt in range(3):  # 3 –ø–æ–ø—ã—Ç–∫–∏
            try:
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "user", "content": PROMPT_TEMPLATE.format(dialog_text=dialog_text)}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 2048
                }

                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60
                )

                if response.status_code == 200:
                    result_text = response.json()["choices"][0]["message"]["content"]
                    result = json.loads(result_text)
                    return result
                elif response.status_code == 429:
                    print(f"‚ö†Ô∏è –õ–∏–º–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model}. –ñ–¥—ë–º 5 —Å–µ–∫...")
                    time.sleep(5)
                    continue
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {response.status_code} –æ—Ç {model}: {response.text}")
                    break  # –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π –º–æ–¥–µ–ª–∏
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ {model} (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}")
                time.sleep(2)
        time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

    return {"error": False, "client_phrase": "", "operator_phrase": "", "error_msg": "–í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∏"}

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
conn = sqlite3.connect(config.DATABASE_PATH)
cursor = conn.cursor()

cursor.execute("""
    CREATE TABLE IF NOT EXISTS callback_phrases (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        phrase TEXT NOT NULL UNIQUE,
        source TEXT NOT NULL,
        category INTEGER NOT NULL,
        frequency INTEGER DEFAULT 1,
        verified BOOLEAN DEFAULT 0
    );
""")

cursor.execute("SELECT id, text FROM dialogs")
dialogs = cursor.fetchall()
print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É...")

def process_single_dialog(dialog_id, text):
    print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–∞ ID: {dialog_id}...")
    result = extract_phrases_with_llm(text)

    if result.get("client_phrase"):
        if result["error"]:
            source = "client_promise"
            category = 1
        else:
            source = "client_uncertain"
            category = 3
        cursor.execute("""
            INSERT OR IGNORE INTO callback_phrases (phrase, source, category, frequency, verified)
            VALUES (?, ?, ?, 1, 0)
        """, (result["client_phrase"], source, category))
        cursor.execute("""
            UPDATE callback_phrases SET frequency = frequency + 1 WHERE phrase = ?
        """, (result["client_phrase"],))
    
    if result.get("operator_phrase"):
        cursor.execute("""
            INSERT OR IGNORE INTO callback_phrases (phrase, source, category, frequency, verified)
            VALUES (?, ?, 2, 1, 0)
        """, (result["operator_phrase"], "operator_offer"))
        cursor.execute("""
            UPDATE callback_phrases SET frequency = frequency + 1 WHERE phrase = ?
        """, (result["operator_phrase"],))

    return dialog_id

# === –ó–ê–ü–£–°–ö –° 55 –ü–û–¢–û–ö–ê–ú–ò ===
with ThreadPoolExecutor(max_workers=55) as executor:
    futures = [executor.submit(process_single_dialog, dialog_id, text) for dialog_id, text in dialogs]
    
    for i, future in enumerate(as_completed(futures), 1):
        dialog_id = future.result()
        if i % 10 == 0:
            conn.commit()
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i} –¥–∏–∞–ª–æ–≥–æ–≤.")

conn.commit()
conn.close()

print("‚úÖ –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –§—Ä–∞–∑—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É 'callback_phrases'.")

===== FILE: gui.py =====
"""GUI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤ call-—Ü–µ–Ω—Ç—Ä–∞ ‚Äî –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–µ–ø–ª–∏–∫–∞–º, HyDE, Reranker, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º QA –∏ —á–∞—Ç–æ–º."""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import json
import numpy as np
import faiss
import sqlite3
import logging
import threading
import requests
import pickle
from datetime import datetime
from pathlib import Path

# === –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —É—Ç–∏–ª–∏—Ç ===
import config
from utils import setup_logger

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logger = setup_logger('GUI', config.LOGS_ROOT / "gui.log")

# === –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π ===
from sentence_transformers import SentenceTransformer
try:
    from sentence_transformers import CrossEncoder
    RERANKER_AVAILABLE = True
    reranker = CrossEncoder('BAAI/bge-reranker-large', max_length=512)
except Exception as e:
    logger.warning(f"Reranker –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    RERANKER_AVAILABLE = False
    reranker = None

# === –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ===
MODEL = None
INDEXES = {}        # {theme: faiss_index}
IDS = {}            # {theme: [utterance_id1, ...]}
DATA_LOOKUPS = {}   # {utterance_id: {text, speaker, dialog_id, turn_order, full_dialog_text}}
CHAT_DB_CONN = None
CURRENT_THEME = "all"

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î –¥–ª—è —á–∞—Ç–∞ –∏ QA ===
def init_chat_db():
    global CHAT_DB_CONN
    try:
        CHAT_DB_CONN = sqlite3.connect(config.DATABASE_PATH, check_same_thread=False)
        logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á–∞—Ç–∞ –∏ QA –ø–æ–¥–∫–ª—é—á–µ–Ω–∞.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î —á–∞—Ç–∞: {e}")
        messagebox.showerror("–û—à–∏–±–∫–∞ –ë–î", f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —á–∞—Ç–∞: {e}")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö ===
def load_models():
    global MODEL
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ embedding-–º–æ–¥–µ–ª–∏: {config.EMBEDDING_MODEL_NAME}")
    MODEL = SentenceTransformer(config.EMBEDDING_MODEL_NAME, device="cpu")
    MODEL.max_seq_length = config.EMBEDDING_MODEL_MAX_LENGTH

def load_faiss_indexes():
    global INDEXES, IDS
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ FAISS-–∏–Ω–¥–µ–∫—Å–æ–≤...")
    conn = sqlite3.connect(config.DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT theme, index_path, ids_path FROM faiss_indexes")
    rows = cursor.fetchall()
    conn.close()
    
    for theme, index_path, ids_path in rows:
        if Path(index_path).exists() and Path(ids_path).exists():
            try:
                INDEXES[theme] = faiss.read_index(index_path)
                with open(ids_path, 'r', encoding='utf-8') as f:
                    IDS[theme] = json.load(f)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {theme} ({len(IDS[theme])} —Ä–µ–ø–ª–∏–∫)")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ {theme}: {e}")

def load_data_lookups():
    global DATA_LOOKUPS
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫ –∏ –¥–∏–∞–ª–æ–≥–æ–≤...")
    conn = sqlite3.connect(config.DATABASE_PATH)
    cursor = conn.cursor()
    
    cursor.execute("SELECT id, dialog_id, speaker, text, turn_order FROM utterances")
    utterances = cursor.fetchall()
    
    cursor.execute("SELECT id, text FROM dialogs")
    dialog_texts = {row[0]: row[1] for row in cursor.fetchall()}
    
    for utterance_id, dialog_id, speaker, text, turn_order in utterances:
        DATA_LOOKUPS[utterance_id] = {
            "text": text,
            "speaker": speaker,
            "dialog_id": dialog_id,
            "turn_order": turn_order,
            "full_dialog_text": dialog_texts.get(dialog_id, "")
        }
    
    conn.close()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(DATA_LOOKUPS)} —Ä–µ–ø–ª–∏–∫.")

# === HyDE ===
def generate_hypothetical_answer(query):
    try:
        hyde_prompt = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ call-—Ü–µ–Ω—Ç—Ä–∞.
–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Ç–æ—á–Ω—ã–π –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã —É–∂–µ –Ω–∞—à—ë–ª –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –¥–∏–∞–ª–æ–≥–æ–≤.

–í–æ–ø—Ä–æ—Å: {query}

–ì–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç:"""
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                "model": config.LLM_MODEL_NAME,
                "prompt": hyde_prompt,
                "stream": False,
                "options": {"temperature": 0.1}
            },
            timeout=30
        )
        if response.status_code == 200:
            result = response.json()
            return result.get("response", "").strip()
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ HyDE: {e}")
    return query

# === Reranker ===
def rerank_results(query, candidates):
    if not RERANKER_AVAILABLE or not candidates:
        return candidates
    try:
        pairs = [(query, cand["text"]) for cand in candidates]
        scores = reranker.predict(pairs)
        for i, score in enumerate(scores):
            candidates[i]["rerank_score"] = float(score)
        candidates.sort(key=lambda x: x["rerank_score"], reverse=True)
        logger.info("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω—ã.")
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ reranker: {e}")
    return candidates

# === –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–ø–ª–∏–∫–∞–º ===
def find_similar_utterances(query, theme="all", top_k=5):
    if theme not in INDEXES:
        logger.error(f"–ò–Ω–¥–µ–∫—Å –¥–ª—è —Ç–µ–º—ã '{theme}' –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        return []
    
    # ‚úÖ –ü—Ä–æ—Å—Ç–æ –∫–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å ‚Äî –±–µ–∑ HyDE
    query_vector = MODEL.encode([query], convert_to_tensor=False)[0].astype('float32')
    
    index = INDEXES[theme]
    ids_list = IDS[theme]
    distances, indices = index.search(np.array([query_vector]), top_k)
    
    candidates = []
    for i, idx in enumerate(indices[0]):
        if idx >= len(ids_list): continue
        utterance_id = ids_list[idx]
        if utterance_id not in DATA_LOOKUPS: continue
        item = DATA_LOOKUPS[utterance_id].copy()
        item["id"] = utterance_id
        item["faiss_score"] = float(distances[0][i])
        candidates.append(item)
    
    # ‚úÖ –ë–µ–∑ reranker
    return candidates

# === –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏ ===
def format_context_for_llm(results):
    context_parts = []
    for item in results:
        dialog_id = item["dialog_id"]
        turn_order = item["turn_order"]
        full_text = item["full_dialog_text"]
        
        surrounding_lines = []
        dialog_lines = full_text.splitlines()
        target_line = f"{item['speaker']}: {item['text']}"
        
        for i, line in enumerate(dialog_lines):
            if line == target_line or (line.startswith(item['speaker'] + ":") and item['text'] in line):
                start = max(0, i - 2)
                end = min(len(dialog_lines), i + 3)
                surrounding_lines = dialog_lines[start:end]
                break
        
        context_parts.append(
            f"[–°—Ö–æ–∂–µ—Å—Ç—å: {item.get('rerank_score', item['faiss_score']):.3f}] ID: {item['id']}\n" +
            "\n".join(surrounding_lines) +
            "\n---"
        )
    return "\n\n".join(context_parts)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ QA-–ø–∞—Ä—ã ===
def save_qa_pair(question, theme, method, params, answer, context_summary):
    try:
        cursor = CHAT_DB_CONN.cursor()
        cursor.execute("""
            INSERT INTO qa_pairs (timestamp, question, theme, method_used, parameters, answer, context_summary)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            question,
            theme,
            method,
            json.dumps(params, ensure_ascii=False),
            answer,
            json.dumps(context_summary, ensure_ascii=False)
        ))
        CHAT_DB_CONN.commit()
        logger.info("‚úÖ –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è QA-–ø–∞—Ä—ã: {e}")

# === –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ analysis_methods.py ===
def run_ask():
    question = entry.get().strip()
    if not question:
        messagebox.showwarning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å!")
        return

    theme = theme_var.get()
    top_k = int(top_k_var.get())
    chunk_size = int(chunk_size_var.get())
    selected_method_name = method_var.get()

    ask_btn.config(state=tk.DISABLED, text="–ò—â—É...")
    status_label.config(text=f"üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫ (—Ç–µ–º–∞: {theme})...")
    root.update_idletasks()

    def update_status(text):
        status_label.config(text=text)
        root.update_idletasks()

    def worker():
        try:
            results = find_similar_utterances(question, theme=theme, top_k=top_k)
            if not results:
                answer_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã."
                context_text = "–ù–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–ø–ª–∏–∫."
            else:
                import analysis_methods
                analysis_func = analysis_methods.get_analysis_method(selected_method_name)
                if not analysis_func:
                    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥: {selected_method_name}")

                context_text = format_context_for_llm(results)
                answer_text = analysis_func(
                    question=question,
                    found_with_scores=[(r, r.get("rerank_score", r["faiss_score"])) for r in results],
                    chunk_size=chunk_size,
                    status_callback=update_status
                )
                
                save_qa_pair(
                    question=question,
                    theme=theme,
                    method=selected_method_name,
                    params={"top_k": top_k, "chunk_size": chunk_size},
                    answer=answer_text,
                    context_summary=[r["id"] for r in results]
                )

            text_answer.delete(1.0, tk.END)
            text_answer.insert(tk.END, answer_text)
            text_context.delete(1.0, tk.END)
            text_context.insert(tk.END, context_text)
            status_label.config(text=f"‚úÖ –ì–æ—Ç–æ–≤. –¢–µ–º–∞: {theme}. –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ø–ª–∏–∫.")
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞: {e}"
            logger.error(error_msg)
            text_answer.delete(1.0, tk.END)
            text_answer.insert(tk.END, error_msg)
            status_label.config(text=f"‚ùå {error_msg}")
        finally:
            ask_btn.config(state=tk.NORMAL, text="–°–ø—Ä–æ—Å–∏—Ç—å")

    thread = threading.Thread(target=worker)
    thread.daemon = True
    thread.start()

# === –≠–∫—Å–ø–æ—Ä—Ç ===
def export_text(widget, default_name):
    content = widget.get(1.0, tk.END)
    if not content.strip():
        messagebox.showwarning("–≠–∫—Å–ø–æ—Ä—Ç", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞.")
        return
    file_path = filedialog.asksaveasfilename(
        defaultextension=".txt",
        initialfile=default_name,
        filetypes=[("Text files", "*.txt"), ("All files", "*.*")]
    )
    if file_path:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        messagebox.showinfo("–≠–∫—Å–ø–æ—Ä—Ç", f"–î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã:\n{file_path}")

def export_answer():
    export_text(text_answer, f"–û—Ç–≤–µ—Ç_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

def export_context():
    export_text(text_context, f"–ö–æ–Ω—Ç–µ–∫—Å—Ç_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# === –ß–∞—Ç —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º ===
def on_send_click():
    user_message = entry_chat.get().strip()
    if not user_message: return
    chat_history.insert(tk.END, f"–í—ã: {user_message}\n")
    chat_history.see(tk.END)
    entry_chat.delete(0, tk.END)
    threading.Thread(target=process_user_message, args=(user_message,), daemon=True).start()

def process_user_message(user_message):
    try:
        if user_message.lower().startswith(("–Ω–∞–π—Ç–∏", "–ø–æ–∏—Å–∫", "–Ω–∞–π–¥–∏", "search")):
            prompt = f"–ó–∞–¥–∞–π 1-2 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{user_message}'"
        else:
            prompt = f"–û—Ç–≤–µ—Ç—å –∫–∞–∫ –∞–Ω–∞–ª–∏—Ç–∏–∫ call-—Ü–µ–Ω—Ç—Ä–∞: {user_message}"
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={"model": config.LLM_MODEL_NAME, "prompt": prompt, "stream": False},
            timeout=30
        )
        if response.status_code == 200:
            answer = response.json().get("response", "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏").strip()
            chat_history.insert(tk.END, f"–ê–Ω–∞–ª–∏—Ç–∏–∫: {answer}\n")
        else:
            chat_history.insert(tk.END, f"–ê–Ω–∞–ª–∏—Ç–∏–∫: –û—à–∏–±–∫–∞ LLM ({response.status_code})\n")
    except Exception as e:
        chat_history.insert(tk.END, f"–ê–Ω–∞–ª–∏—Ç–∏–∫: –û—à–∏–±–∫–∞: {e}\n")
    chat_history.see(tk.END)

# === GUI ===
def create_gui():
    global root, status_label, ask_btn, btn_send, entry, text_answer, text_context
    global top_k_var, chunk_size_var, method_var, theme_var, entry_chat, chat_history, theme_menu
    global export_answer_btn, export_context_btn

    root = tk.Tk()
    root.title("CallCenter AI –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä v3 (—Ä–µ–ø–ª–∏–∫–∏ + HyDE + Reranker)")
    root.geometry("1350x900")

    # –¢–µ–º–Ω–∞—è —Ç–µ–º–∞
    theme_colors = config.GUI_THEME
    dark_bg = theme_colors["dark_bg"]
    dark_fg = theme_colors["dark_fg"]
    dark_entry_bg = theme_colors["dark_entry_bg"]
    dark_button_bg = theme_colors["dark_button_bg"]
    dark_frame_bg = theme_colors["dark_frame_bg"]
    success_color = theme_colors["success_color"]

    root.configure(bg=dark_bg)
    style = ttk.Style()
    style.theme_use('clam')
    style.configure("TFrame", background=dark_frame_bg)
    style.configure("TLabel", background=dark_bg, foreground=dark_fg)
    style.configure("TButton", background=dark_button_bg, foreground=dark_fg)
    style.configure("TCombobox", fieldbackground=dark_entry_bg, background=dark_button_bg, foreground=dark_fg)
    style.configure("TNotebook", background=dark_bg)
    style.configure("TNotebook.Tab", background=dark_button_bg, foreground=dark_fg)
    style.map("TNotebook.Tab", background=[('selected', dark_bg)], foreground=[('selected', dark_fg)])

    # –í–µ—Ä—Ö–Ω—è—è –ø–∞–Ω–µ–ª—å
    frame_top = tk.Frame(root, bg=dark_frame_bg)
    frame_top.pack(pady=10, padx=10, fill=tk.X)

    tk.Label(frame_top, text="–í–∞—à –≤–æ–ø—Ä–æ—Å:", bg=dark_frame_bg, fg=dark_fg, font=('Arial', 10, 'bold')).pack(anchor=tk.W)
    entry = tk.Entry(frame_top, width=70, font=('Arial', 10), bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))

    settings_frame = tk.Frame(frame_top, bg=dark_frame_bg)
    settings_frame.pack(side=tk.LEFT, padx=(10, 5))

    tk.Label(settings_frame, text="–¢–µ–º–∞:", bg=dark_frame_bg, fg=dark_fg).pack(side=tk.LEFT)
    theme_var = tk.StringVar(value="–ó–∞–≥—Ä—É–∑–∫–∞...")
    theme_menu = ttk.Combobox(settings_frame, textvariable=theme_var, values=["–ó–∞–≥—Ä—É–∑–∫–∞..."], state="readonly", width=15)
    theme_menu.pack(side=tk.LEFT, padx=2)

    tk.Label(settings_frame, text="–ú–µ—Ç–æ–¥:", bg=dark_frame_bg, fg=dark_fg).pack(side=tk.LEFT)
    method_var = tk.StringVar(value=config.GUI_DEFAULT_METHOD)
    method_menu = ttk.Combobox(settings_frame, textvariable=method_var, values=config.ANALYSIS_METHODS, state="readonly", width=12)
    method_menu.pack(side=tk.LEFT, padx=2)

    tk.Label(settings_frame, text="–ù–∞–π—Ç–∏:", bg=dark_frame_bg, fg=dark_fg).pack(side=tk.LEFT)
    top_k_var = tk.StringVar(value=str(config.GUI_DEFAULT_TOP_K))
    top_k_spinbox = tk.Spinbox(settings_frame, from_=1, to=10000, textvariable=top_k_var, width=6, bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    top_k_spinbox.pack(side=tk.LEFT, padx=2)

    tk.Label(settings_frame, text="—Ä–µ–ø–ª–∏–∫, –ø–æ", bg=dark_frame_bg, fg=dark_fg).pack(side=tk.LEFT)
    chunk_size_var = tk.StringVar(value=str(config.GUI_DEFAULT_CHUNK_SIZE))
    chunk_size_spinbox = tk.Spinbox(settings_frame, from_=1, to=1000, textvariable=chunk_size_var, width=4, bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    chunk_size_spinbox.pack(side=tk.LEFT, padx=2)

    tk.Label(settings_frame, text="—à—Ç.", bg=dark_frame_bg, fg=dark_fg).pack(side=tk.LEFT)

    ask_btn = tk.Button(frame_top, text="–°–ø—Ä–æ—Å–∏—Ç—å", command=run_ask, state=tk.DISABLED, bg=success_color, fg='white')
    ask_btn.pack(side=tk.RIGHT)

    export_frame = tk.Frame(frame_top, bg=dark_frame_bg)
    export_frame.pack(side=tk.RIGHT, padx=(0, 10))
    export_answer_btn = tk.Button(export_frame, text="–≠–∫—Å–ø–æ—Ä—Ç –û—Ç–≤–µ—Ç–∞", command=export_answer, state=tk.DISABLED, bg=dark_button_bg, fg=dark_fg)
    export_answer_btn.pack(side=tk.LEFT, padx=2)
    export_context_btn = tk.Button(export_frame, text="–≠–∫—Å–ø–æ—Ä—Ç –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞", command=export_context, state=tk.DISABLED, bg=dark_button_bg, fg=dark_fg)
    export_context_btn.pack(side=tk.LEFT, padx=2)

    status_label = tk.Label(root, text="‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞...", bd=1, relief=tk.SUNKEN, anchor=tk.W, bg=dark_entry_bg, fg=dark_fg)
    status_label.pack(side=tk.BOTTOM, fill=tk.X)

    # –í–∫–ª–∞–¥–∫–∏
    notebook = ttk.Notebook(root)
    notebook.pack(pady=10, padx=10, expand=True, fill=tk.BOTH)

    tab_answer = ttk.Frame(notebook)
    notebook.add(tab_answer, text='–û—Ç–≤–µ—Ç')
    text_answer = scrolledtext.ScrolledText(tab_answer, wrap=tk.WORD, font=('Arial', 10), bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    text_answer.pack(padx=5, pady=5, fill=tk.BOTH, expand=True)

    tab_context = ttk.Frame(notebook)
    notebook.add(tab_context, text='–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ø–ª–∏–∫–∏')
    text_context = scrolledtext.ScrolledText(tab_context, wrap=tk.WORD, font=('Arial', 10), bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    text_context.pack(padx=5, pady=5, fill=tk.BOTH, expand=True)

    tab_chat = ttk.Frame(notebook)
    notebook.add(tab_chat, text='–ß–∞—Ç —Å –ê–Ω–∞–ª–∏—Ç–∏–∫–æ–º')
    chat_frame = tk.Frame(tab_chat, bg=dark_frame_bg)
    chat_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
    chat_history = scrolledtext.ScrolledText(chat_frame, wrap=tk.WORD, font=('Arial', 10), bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    chat_history.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
    input_frame = tk.Frame(chat_frame, bg=dark_frame_bg)
    input_frame.pack(fill=tk.X, padx=5, pady=5)
    entry_chat = tk.Entry(input_frame, font=('Arial', 10), bg=dark_entry_bg, fg=dark_fg, insertbackground=dark_fg)
    entry_chat.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))
    btn_send = tk.Button(input_frame, text="–û—Ç–ø—Ä–∞–≤–∏—Ç—å", command=on_send_click, state=tk.DISABLED, bg=dark_button_bg, fg=dark_fg)
    btn_send.pack(side=tk.RIGHT)

    # –ó–∞–≥—Ä—É–∑–∫–∞
    def delayed_init():
        init_chat_db()
        load_models()
        load_faiss_indexes()
        load_data_lookups()
        themes_for_combo = ["all"] + [t for t in INDEXES.keys() if t != "all"]
        theme_menu['values'] = themes_for_combo
        if themes_for_combo:
            theme_var.set(themes_for_combo[0])
        ask_btn.config(state=tk.NORMAL)
        btn_send.config(state=tk.NORMAL)
        export_answer_btn.config(state=tk.NORMAL)
        export_context_btn.config(state=tk.NORMAL)
        status_label.config(text="‚úÖ –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

    root.after(100, delayed_init)
    root.mainloop()

if __name__ == "__main__":
    create_gui()

===== FILE: indexer.py =====
"""–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–µ–ø–ª–∏–∫ (utterances) –≤ FAISS. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤.
   –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω: –±–∞—Ç—á–∏, –ª–æ–≥–∏, –ø—Ä–æ–≤–µ—Ä–∫–∞, —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏."""

import os
import json
import pickle
import sqlite3
import logging
from pathlib import Path
from datetime import datetime  # –ò–º–ø–æ—Ä—Ç –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è
import faiss
import numpy as np
from tqdm import tqdm

# === –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
import config

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(config.LOGS_ROOT / "indexer.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("Indexer")

BATCH_SIZE = 1024  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ FAISS

def get_themes_from_db(conn):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º –∏–∑ –ë–î."""
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT source_theme FROM dialogs WHERE source_theme IS NOT NULL")
    themes = [row[0] for row in cursor.fetchall() if row[0]]
    return themes

def build_faiss_index_for_theme(theme_name, conn, index_path, ids_path):
    """–°—Ç—Ä–æ–∏—Ç FAISS-–∏–Ω–¥–µ–∫—Å –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º—ã (–∏–ª–∏ 'all') –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ø–ª–∏–∫."""
    logger.info(f"üîç –ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Ç–µ–º—ã: '{theme_name}'...")

    # –ó–∞–ø—Ä–æ—Å: –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –¥–ª—è —Ç–µ–º—ã
    if theme_name == "all":
        count_query = "SELECT COUNT(*) FROM utterance_embeddings ue JOIN utterances u ON ue.utterance_id = u.id"
        fetch_query = """
            SELECT ue.utterance_id, ue.vector
            FROM utterance_embeddings ue
            JOIN utterances u ON ue.utterance_id = u.id
        """
    else:
        count_query = """
            SELECT COUNT(*)
            FROM utterance_embeddings ue
            JOIN utterances u ON ue.utterance_id = u.id
            JOIN dialogs d ON u.dialog_id = d.id
            WHERE d.source_theme = ?
        """
        fetch_query = """
            SELECT ue.utterance_id, ue.vector
            FROM utterance_embeddings ue
            JOIN utterances u ON ue.utterance_id = u.id
            JOIN dialogs d ON u.dialog_id = d.id
            WHERE d.source_theme = ?
        """

    cursor = conn.cursor()
    cursor.execute(count_query, (theme_name,) if theme_name != "all" else ())
    total = cursor.fetchone()[0]

    if total == 0:
        logger.warning(f"‚ö†Ô∏è –î–ª—è —Ç–µ–º—ã '{theme_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
        return False

    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {total} —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")

    # –°–æ–∑–¥–∞—ë–º FAISS –∏–Ω–¥–µ–∫—Å
    dimension = config.EMBEDDING_MODEL_DIMENSION
    index = faiss.IndexFlatIP(dimension)  # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ IndexIVFFlat –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–æ–≤

    utterance_ids = []
    offset = 0

    pbar = tqdm(total=total, desc=f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è '{theme_name}'", unit="—Ä–µ–ø–ª–∏–∫–∞")

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π
    while offset < total:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        if theme_name == "all":
            params = (BATCH_SIZE, offset)
        else:
            params = (theme_name, BATCH_SIZE, offset)

        cursor.execute(fetch_query + " LIMIT ? OFFSET ?", params)
        rows = cursor.fetchall()

        if not rows:
            break

        vectors = []
        batch_ids = []

        for row in rows:
            utterance_id, vector_blob = row
            try:
                vector = pickle.loads(vector_blob)
                if isinstance(vector, np.ndarray):
                    vector = vector.astype('float32')
                else:
                    vector = np.array(vector, dtype='float32')
                vectors.append(vector)
                batch_ids.append(utterance_id)
            except Exception as e:
                logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–∞ {utterance_id}: {e}")
                continue

        if vectors:
            vectors = np.array(vectors)
            faiss.normalize_L2(vectors)
            index.add(vectors)
            utterance_ids.extend(batch_ids)

        pbar.update(len(batch_ids))
        offset += BATCH_SIZE

    pbar.close()

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏ ID ---
    try:
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(os.path.dirname(index_path), exist_ok=True)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å (–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Path –≤ str)
        faiss.write_index(index, str(index_path))
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ ID
        with open(ids_path, 'w', encoding='utf-8') as f:
            json.dump(utterance_ids, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –¥–ª—è '{theme_name}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ –¥–∏—Å–∫: {index_path}, {len(utterance_ids)} —Ä–µ–ø–ª–∏–∫.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞/ID –¥–ª—è '{theme_name}' –Ω–∞ –¥–∏—Å–∫: {e}")
        return False

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞ –≤ –ë–î ---
    try:
        meta_cursor = conn.cursor()
        meta_cursor.execute("""
            INSERT OR REPLACE INTO faiss_indexes (theme, index_path, ids_path, built_at)
            VALUES (?, ?, ?, ?)
        """, (theme_name, str(index_path), str(ids_path), datetime.now().isoformat()))
        conn.commit()
        logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è '{theme_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è '{theme_name}' –≤ –ë–î: {e}")
        conn.rollback() # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º, –µ—Å–ª–∏ —ç—Ç–æ –±—ã–ª–æ –≤–Ω—É—Ç—Ä–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False, —Ç–∞–∫ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫–µ —É–∂–µ —Å–æ–∑–¥–∞–Ω

    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤."""
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è FAISS-–∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Ä–µ–ø–ª–∏–∫...")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    conn = sqlite3.connect(config.DATABASE_PATH)
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ–º
    themes = get_themes_from_db(conn)
    themes.append("all")  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é

    # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
    for theme in themes:
        index_path = config.FAISS_INDEX_DIR / f"faiss_index_{theme}.index"
        ids_path = config.FAISS_INDEX_DIR / f"ids_{theme}.json"
        
        success = build_faiss_index_for_theme(theme, conn, index_path, ids_path)
        if not success:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ç–µ–º—ã '{theme}'.")

    conn.close()
    logger.info("üèÅ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

if __name__ == "__main__":
    main()

===== FILE: init_db.py =====
# init_db.py
import sqlite3
import os

# –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DB_PATH = "database.db"  # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —ç—Ç–æ —Ç–æ—Ç –∂–µ –ø—É—Ç—å, —á—Ç–æ –∏ —É —Ç–µ–±—è

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# –í–∫–ª—é—á–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É foreign keys
cursor.execute("PRAGMA foreign_keys = ON;")

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
tables_sql = [
    """
    CREATE TABLE IF NOT EXISTS dialogs (
        id TEXT PRIMARY KEY,
        text TEXT NOT NULL,
        metadata TEXT NOT NULL,
        source_theme TEXT NOT NULL,
        processed_at TEXT NOT NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS utterances (
        id TEXT PRIMARY KEY,
        dialog_id TEXT NOT NULL,
        speaker TEXT NOT NULL,
        text TEXT NOT NULL,
        turn_order INTEGER NOT NULL,
        FOREIGN KEY (dialog_id) REFERENCES dialogs(id)
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS utterance_embeddings (
        utterance_id TEXT PRIMARY KEY,
        vector BLOB NOT NULL,
        FOREIGN KEY (utterance_id) REFERENCES utterances(id)
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS embeddings (
        dialog_id TEXT PRIMARY KEY,
        vector BLOB NOT NULL,
        FOREIGN KEY (dialog_id) REFERENCES dialogs(id)
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS faiss_indexes (
        theme TEXT PRIMARY KEY,
        index_path TEXT NOT NULL,
        ids_path TEXT NOT NULL,
        built_at TEXT NOT NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS callback_phrases (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        phrase TEXT NOT NULL UNIQUE,
        source TEXT NOT NULL,
        category INTEGER NOT NULL,
        frequency INTEGER DEFAULT 1,
        verified BOOLEAN DEFAULT 0
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS qa_pairs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        timestamp TEXT NOT NULL,
        question TEXT NOT NULL,
        theme TEXT NOT NULL,
        method_used TEXT NOT NULL,
        parameters TEXT NOT NULL,
        answer TEXT NOT NULL,
        context_summary TEXT NOT NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS raw_llm_analyses (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        dialog_id TEXT NOT NULL,
        analysis_result TEXT NOT NULL,
        timestamp TEXT NOT NULL,
        FOREIGN KEY (dialog_id) REFERENCES dialogs(id)
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS chat_history (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        timestamp TEXT NOT NULL,
        user_message TEXT NOT NULL,
        bot_response TEXT NOT NULL
    );
    """,

    """
    CREATE TABLE IF NOT EXISTS search_cache (
        query_hash TEXT PRIMARY KEY,
        results TEXT NOT NULL
    );
    """
]

# –í—ã–ø–æ–ª–Ω—è–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
for sql in tables_sql:
    cursor.execute(sql)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
conn.commit()
conn.close()

print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{DB_PATH}' –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã.")

===== FILE: pack_project.py =====
import os
from pathlib import Path

EXCLUDE = {".venv", "venv", "__pycache__", ".git", ".idea", "node_modules"}

def should_skip(path):
    return any(part in str(path) for part in EXCLUDE)

def pack_project(root="."):
    output = []
    output.append("=== PROJECT STRUCTURE ===")
    for path in sorted(Path(root).rglob("*")):
        if path.is_dir() and not should_skip(path):
            output.append(f"[DIR]  {path}")
        elif path.is_file() and path.suffix == ".py" and not should_skip(path):
            output.append(f"[FILE] {path}")

    output.append("\n=== PYTHON FILES CONTENT ===")
    for file in sorted(Path(root).rglob("*.py")):
        if not should_skip(file):
            output.append(f"\n===== FILE: {file} =====")
            try:
                output.append(file.read_text(encoding="utf-8"))
            except Exception as e:
                output.append(f"# –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")

    Path("project_packed.txt").write_text("\n".join(output), encoding="utf-8")
    print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ project_packed.txt")

if __name__ == "__main__":
    pack_project()

===== FILE: phrase_stats.py =====
import json

with open('aggregated_phrases.json', 'r', encoding='utf-8') as f:
    agg = json.load(f)

with open('callback_results.json', 'r', encoding='utf-8') as f:
    results = json.load(f)

# –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
cat_counts = {1:0, 2:0, 3:0, 4:0}
for item in results:
    cat_counts[item["category"]] += 1

print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
for cat in [1,2,3,4]:
    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {cat}: {cat_counts[cat]} –¥–∏–∞–ª–æ–≥–æ–≤")

print("\nüî§ –£–ù–ò–ö–ê–õ–¨–ù–´–ï –§–†–ê–ó–´:")
for cat_key in agg:
    client_count = len(agg[cat_key]["client"])
    op_count = len(agg[cat_key]["operator"])
    print(f"{cat_key}: –∫–ª–∏–µ–Ω—Ç ‚Äî {client_count}, –æ–ø–µ—Ä–∞—Ç–æ—Ä ‚Äî {op_count}")

# –¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ñ—Ä–∞–∑ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 1 (–∫–ª–∏–µ–Ω—Ç)
from collections import Counter
cat1_client_phrases = [item["client_phrases"] for item in results if item["category"] == 1]
all_cat1_client = [phrase for sublist in cat1_client_phrases for phrase in sublist]
phrase_freq = Counter(all_cat1_client).most_common(10)

print("\nüîù –¢–û–ü-10 –§–†–ê–ó –ö–õ–ò–ï–ù–¢–ê –í –ö–ê–¢–ï–ì–û–†–ò–ò 1 (–æ—à–∏–±–∫–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞):")
for phrase, count in phrase_freq:
    print(f"{count:3d}x | {phrase}")

===== FILE: pipeline.py =====
"""–û–±—Ä–∞–±–æ—Ç–∫–∞ RTF-—Ñ–∞–π–ª–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (sentence-transformers GPU batch) –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite.
   –û–ë–ù–û–í–õ–ï–ù–û: —Ç–µ–ø–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–ø–ª–∏–∫–∏ (utterances) –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç–¥–µ–ª—å–Ω–æ.
   + –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –±–∞—Ç—á, TF32, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞, PYTORCH_CUDA_ALLOC_CONF.
"""

import os
import json
import sqlite3
import hashlib
import logging
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import gc

# === –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PYTORCH_CUDA_ALLOC_CONF –î–û –∏–º–ø–æ—Ä—Ç–∞ torch ===
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# === sentence-transformers –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ ===
from sentence_transformers import SentenceTransformer
import torch

# === –í–∫–ª—é—á–∏—Ç—å TF32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è ===
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# === –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ RTF ===
from striprtf.striprtf import rtf_to_text

# === –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
import config

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
from utils import setup_logger
logger = setup_logger('Pipeline', config.LOGS_ROOT / "pipeline.log")

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)
console_formatter = logging.Formatter('üö® %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)
logger.propagate = False

def log_to_file_only(msg):
    logger.info(msg)

def log(msg):
    logger.info(msg)

# === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î ===
def ensure_db_initialized():
    log_to_file_only("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ë–î...")
    try:
        conn = sqlite3.connect(config.DATABASE_PATH)
        cursor = conn.cursor()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ dialogs
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='dialogs';")
        if cursor.fetchone() is None:
            log_to_file_only("–¢–∞–±–ª–∏—Ü—ã –≤ –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏...")
            import init_db
            init_db.init_db()
            print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ë–î –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã utterances, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS utterances (
                id TEXT PRIMARY KEY,
                dialog_id TEXT NOT NULL,
                speaker TEXT NOT NULL,
                text TEXT NOT NULL,
                turn_order INTEGER NOT NULL,
                FOREIGN KEY (dialog_id) REFERENCES dialogs(id) ON DELETE CASCADE
            );
        """)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã utterance_embeddings, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS utterance_embeddings (
                utterance_id TEXT PRIMARY KEY,
                vector BLOB NOT NULL,
                FOREIGN KEY (utterance_id) REFERENCES utterances(id) ON DELETE CASCADE
            );
        """)
        
        conn.commit()
        conn.close()
        log_to_file_only("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ë–î –∏ —Ç–∞–±–ª–∏—Ü utterances/utterance_embeddings –≤ –ø–æ—Ä—è–¥–∫–µ.")
    except Exception as e:
        log_to_file_only(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
        raise e

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ sentence-transformers ===
def load_embedding_model():
    log_to_file_only(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {config.EMBEDDING_MODEL_NAME} –Ω–∞ {config.EMBEDDING_MODEL_DEVICE}...")
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {config.EMBEDDING_MODEL_NAME} –Ω–∞ {config.EMBEDDING_MODEL_DEVICE}...")
    
    device = torch.device(config.EMBEDDING_MODEL_DEVICE if torch.cuda.is_available() or config.EMBEDDING_MODEL_DEVICE != "cuda" else "cpu")
    
    model_kwargs = {}
    if config.EMBEDDING_MODEL_PRECISION == "float16" and device.type == "cuda":
        model_kwargs["torch_dtype"] = torch.float16
        
    model = SentenceTransformer(config.EMBEDDING_MODEL_NAME, device=str(device), model_kwargs=model_kwargs)
    model.max_seq_length = config.EMBEDDING_MODEL_MAX_LENGTH
    
    log_to_file_only(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}, —Ç–æ—á–Ω–æ—Å—Ç—å: {config.EMBEDDING_MODEL_PRECISION}")
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}, —Ç–æ—á–Ω–æ—Å—Ç—å: {config.EMBEDDING_MODEL_PRECISION}")
    return model, device

def hash_file(path: Path) -> str:
    return hashlib.md5(path.read_bytes()).hexdigest()

def extract_metadata_from_filename(filename_stem: str) -> dict:
    import re
    pattern = r'^(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})_([A-Za-z0-9_]+)_([0-9]+)$'
    match = re.match(pattern, filename_stem)
    if match:
        year, month, day, hour, minute, second, operator_login, client_number = match.groups()
        if re.match(r'^\d{10,11}$', client_number):
            return {
                "date_from_filename": f"{year}-{month}-{day}",
                "time_from_filename": f"{hour}:{minute}:{second}",
                "operator_login_from_filename": operator_login,
                "client_number_from_filename": client_number
            }
    return {}

def clean_dialog_text_no_filter(text, file_metadata_from_name, file_id):
    import re
    from datetime import datetime
    lines = [line.rstrip() for line in text.splitlines() if line.rstrip()]
    if not lines:
        return [], file_metadata_from_name
    dialog_id = file_id
    dialog_datetime = None
    participants_line = None
    participants = {}
    dialog_type = "unknown"
    for i, line in enumerate(lines):
        id_datetime_match = re.match(r'^(\d+)\s*\((\d{2}\.\d{2}\.\d{4}\s\d{1,2}:\d{2}:\d{2})\)', line.strip())
        if id_datetime_match:
            dialog_id = id_datetime_match.group(1)
            try:
                dt_obj = datetime.strptime(id_datetime_match.group(2), '%d.%m.%Y %H:%M:%S')
                dialog_datetime = dt_obj.isoformat()
                dialog_type = "voice"
            except ValueError:
                pass
            if i + 1 < len(lines):
                participants_line = lines[i + 1].strip()
                p_match = re.search(r'([A-Za-z0-9@._\-]+)\s*(->|<-)\s*([0-9a-fA-F\-]+)', participants_line)
                if p_match:
                    op_raw = p_match.group(1)
                    arrow = p_match.group(2)
                    client_raw = p_match.group(3)
                    op_login = op_raw.split('@')[0] if '@' in op_raw else op_raw
                    participants["operator_login"] = op_login
                    participants["operator_raw"] = op_raw
                    participants["arrow"] = arrow
                    participants["client_id"] = client_raw
                    if client_raw.isdigit() and len(client_raw) >= 10:
                        participants["client_number"] = client_raw
            break
    if lines and "Rtf export" in lines[0] and "call(s)" in lines[0]:
        dialog_type = "chat"
    dialog_lines = []
    i = 0
    if participants_line:
        try:
            start_idx = lines.index(participants_line) + 1
            i = start_idx
        except ValueError:
            i = 0
    while i < len(lines):
        line = lines[i]
        if line.strip().lower() in ["rtf export, 1 call(s)", "–ø–æ—Ä–æ–≥ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 5,00 —Å"]:
            i += 1
            continue
        if re.match(r'^\d+\s*\(\d{2}\.\d{2}\.\d{4}\s\d{1,2}:\d{2}:\d{2}\)', line.strip()) and i > 2:
             break
        time_match = re.search(r'\t(\d+:\d+:\d+)$', line)
        line_text = line
        line_time = None
        if time_match:
            line_time = time_match.group(1)
            line_text = line[:time_match.start()].rstrip("\t")
        line_text = line_text.strip()
        if not line_text:
            i += 1
            continue
        parts = line_text.split('\t', 1)
        potential_speaker = parts[0].strip()
        replica_text = parts[1].strip() if len(parts) > 1 else line_text
        if potential_speaker and len(potential_speaker) < 100 and replica_text:
            speaker = potential_speaker.split('@')[0] if '@' in potential_speaker else potential_speaker
            time_str = f" [{line_time}]" if line_time else ""
            dialog_lines.append(f"{speaker}: {replica_text}{time_str}")
        elif dialog_lines and replica_text:
            dialog_lines[-1] += f" {replica_text}"
        else:
             dialog_lines.append(line_text)
        i += 1

    final_metadata = file_metadata_from_name.copy()
    final_metadata.update({
        "dialog_id": dialog_id,
        "dialog_datetime": dialog_datetime,
        "participants_raw": participants_line,
        "dialog_type": dialog_type,
        "participants": participants
    })
    return dialog_lines, final_metadata  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–ø–ª–∏–∫

# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===
def process_thematic_folders():
    ensure_db_initialized()
    MODEL, device = load_embedding_model()
    
    log_to_file_only("üîç –ü–æ–∏—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞–ø–æ–∫ –≤ Input...")
    theme_folders = [f for f in config.INPUT_ROOT.iterdir() if f.is_dir()]

    if not theme_folders:
        msg = "‚ö†Ô∏è –í –ø–∞–ø–∫–µ 'Input' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥–ø–∞–ø–æ–∫."
        log_to_file_only(msg)
        print(msg)
        return

    msg = f"üìÅ –ù–∞–π–¥–µ–Ω—ã —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞–ø–∫–∏: {[f.name for f in theme_folders]}"
    log_to_file_only(msg)
    print(msg)

    conn = sqlite3.connect(config.DATABASE_PATH)
    cursor = conn.cursor()

    total_new_dialogs = 0
    
    for theme_folder in theme_folders:
        theme_name = theme_folder.name
        msg = f"üìÇ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–º—ã: {theme_name}"
        log_to_file_only(msg)
        print(msg)
        
        theme_proc_path = config.PROCESSED_ROOT / theme_name
        theme_proc_path.mkdir(parents=True, exist_ok=True)
        
        already_processed_files = {hash_file(p): p.stem for p in theme_proc_path.glob("*.rtf")}
        files_to_process = [p for p in theme_folder.glob("*.rtf") if hash_file(p) not in already_processed_files]
        
        if not files_to_process:
            msg = f"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –≤ '{theme_name}' —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã."
            log_to_file_only(msg)
            print(msg)
            continue

        msg = f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(files_to_process)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ '{theme_name}' –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."
        log_to_file_only(msg)

        BATCH_SIZE = config.PIPELINE_BATCH_SIZE
        for i in tqdm(range(0, len(files_to_process), BATCH_SIZE), desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ '{theme_name}' (–±–∞—Ç—á–∞–º–∏ –ø–æ {BATCH_SIZE})", unit="–±–∞—Ç—á"):
            batch_files = files_to_process[i:i + BATCH_SIZE]
            batch_dialogs_to_save = []
            batch_utterances_to_save = []
            batch_texts_to_encode = []
            batch_ids_for_embeddings = []
            
            # –≠—Ç–∞–ø 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
            for file in batch_files:
                try:
                    log_to_file_only(f"  üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file.name}")
                    file_metadata = extract_metadata_from_filename(file.stem)
                    file_metadata["source_theme"] = theme_name
                    
                    raw_text = rtf_to_text(file.read_text(encoding="utf-8", errors="ignore"))
                    file_hash = hash_file(file)[:16]
                    dialog_lines, dialog_metadata = clean_dialog_text_no_filter(raw_text, file_metadata, file_hash)
                    
                    final_metadata = file_metadata.copy()
                    final_metadata.update(dialog_metadata)
                    dialog_id = final_metadata.get("dialog_id", file_hash)
                    
                    cursor.execute("SELECT id FROM dialogs WHERE id = ?", (dialog_id,))
                    if cursor.fetchone() is None:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–ª–æ–≥
                        batch_dialogs_to_save.append({
                            "id": dialog_id,
                            "text": "\n".join(dialog_lines),
                            "metadata": json.dumps(final_metadata, ensure_ascii=False),
                            "source_theme": theme_name,
                            "processed_at": datetime.now().isoformat()
                        })
                        total_new_dialogs += 1
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —Ä–µ–ø–ª–∏–∫—É
                        for idx, line in enumerate(dialog_lines):
                            if ": " in line:
                                speaker_part, text_part = line.split(": ", 1)
                                if " [" in text_part:
                                    text_part = text_part.split(" [")[0]
                            else:
                                speaker_part = "Unknown"
                                text_part = line
                            
                            utterance_id = f"{dialog_id}_u{idx+1:03d}"
                            batch_utterances_to_save.append({
                                "id": utterance_id,
                                "dialog_id": dialog_id,
                                "speaker": speaker_part,
                                "text": text_part,
                                "turn_order": idx + 1
                            })
                            batch_texts_to_encode.append(text_part)
                            batch_ids_for_embeddings.append(utterance_id)
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
                    file.rename(theme_proc_path / file.name)
                        
                except Exception as e:
                    error_msg = f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file.name}: {e}"
                    log_to_file_only(error_msg)
                    print(error_msg)
                    if file.exists():
                        file.rename(theme_proc_path / file.name)

            # –≠—Ç–∞–ø 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤
            if batch_dialogs_to_save:
                try:
                    cursor.executemany("""
                        INSERT INTO dialogs (id, text, metadata, source_theme, processed_at)
                        VALUES (:id, :text, :metadata, :source_theme, :processed_at)
                    """, batch_dialogs_to_save)
                    conn.commit()
                except Exception as e:
                    log_to_file_only(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤: {e}")
                    conn.rollback()

            # –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–ø–ª–∏–∫
            if batch_utterances_to_save:
                try:
                    cursor.executemany("""
                        INSERT INTO utterances (id, dialog_id, speaker, text, turn_order)
                        VALUES (:id, :dialog_id, :speaker, :text, :turn_order)
                    """, batch_utterances_to_save)
                    conn.commit()
                except Exception as e:
                    log_to_file_only(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–ø–ª–∏–∫: {e}")
                    conn.rollback()

            # === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –î–õ–ò–ù –†–ï–ü–õ–ò–ö ===
            if batch_texts_to_encode:
                lengths = [len(text) for text in batch_texts_to_encode]
                log_to_file_only(f"  üìè –î–ª–∏–Ω—ã —Ä–µ–ø–ª–∏–∫: max={max(lengths)}, avg={sum(lengths)/len(lengths):.1f}")

            # –≠—Ç–∞–ø 4: –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–µ–ø–ª–∏–∫
            if batch_texts_to_encode:
                current_batch_size = min(BATCH_SIZE, len(batch_texts_to_encode))
                while current_batch_size > 0:
                    try:
                        log_to_file_only(f"  üß† –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ {len(batch_texts_to_encode)} —Ä–µ–ø–ª–∏–∫ –Ω–∞ {device} —Å –±–∞—Ç—á–µ–º {current_batch_size}...")
                        with torch.no_grad():
                            batch_embeddings = MODEL.encode(
                                batch_texts_to_encode,
                                convert_to_tensor=False,
                                show_progress_bar=False,
                                device=device,
                                batch_size=current_batch_size
                            )
                        break  # –£—Å–ø–µ—à–Ω–æ
                    except torch.cuda.OutOfMemoryError as e:
                        log_to_file_only(f"  ‚ö†Ô∏è CUDA OOM –ø—Ä–∏ –±–∞—Ç—á–µ {current_batch_size}. –ü—Ä–æ–±—É–µ–º —É–º–µ–Ω—å—à–∏—Ç—å...")
                        current_batch_size = max(1, current_batch_size // 2)
                        torch.cuda.empty_cache()
                        gc.collect()
                else:
                    log_to_file_only("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–ª–∏–∫–∏ –¥–∞–∂–µ —Å –±–∞—Ç—á–µ–º 1.")
                    conn.rollback()
                    continue

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                try:
                    import pickle
                    embeddings_to_save = []
                    for uid, emb in zip(batch_ids_for_embeddings, batch_embeddings):
                        emb_blob = pickle.dumps(emb)
                        embeddings_to_save.append((uid, emb_blob))
                    
                    cursor.executemany("""
                        INSERT OR REPLACE INTO utterance_embeddings (utterance_id, vector)
                        VALUES (?, ?)
                    """, embeddings_to_save)
                    conn.commit()
                    
                    if device.type == "cuda":
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                except Exception as e:
                    log_to_file_only(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–µ–ø–ª–∏–∫: {e}")
                    conn.rollback()

        msg = f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º—ã: {theme_name}."
        log_to_file_only(msg)
        print(msg)

    conn.close()
    final_msg = f"üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞–ø–æ–∫. –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤: {total_new_dialogs}"
    log_to_file_only(final_msg)
    print(final_msg)

if __name__ == "__main__":
    process_thematic_folders()

===== FILE: retry_failed_batches.py =====
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""

import sqlite3
import config
import os
from pathlib import Path

def find_and_retry():
    conn = sqlite3.connect(config.DATABASE_PATH)
    cursor = conn.cursor()

    # –ù–∞–π—Ç–∏ –¥–∏–∞–ª–æ–≥–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Ä–µ–ø–ª–∏–∫–∏, –Ω–æ –Ω–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    cursor.execute("""
        SELECT DISTINCT u.dialog_id
        FROM utterances u
        LEFT JOIN utterance_embeddings ue ON u.id = ue.utterance_id
        WHERE ue.utterance_id IS NULL
    """)
    dialog_ids = [row[0] for row in cursor.fetchall()]

    if not dialog_ids:
        print("‚úÖ –í—Å–µ –¥–∏–∞–ª–æ–≥–∏ –∏–º–µ—é—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.")
        return

    print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(dialog_ids)} –¥–∏–∞–ª–æ–≥–æ–≤ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª—ã –æ–±—Ä–∞—Ç–Ω–æ...")

    # –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å —Ñ–∞–π–ª—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ Input
    for dialog_id in dialog_ids:
        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ metadata
        cursor.execute("SELECT metadata FROM dialogs WHERE id = ?", (dialog_id,))
        row = cursor.fetchone()
        if not row:
            continue
        try:
            meta = json.loads(row[0])
            theme = meta.get("source_theme", "unknown")
            filename = f"{dialog_id}.rtf"  # –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
            proc_path = config.PROCESSED_ROOT / theme / filename
            input_path = config.INPUT_ROOT / theme / filename
            if proc_path.exists():
                proc_path.rename(input_path)
                print(f"  –ü–µ—Ä–µ–º–µ—â—ë–Ω: {filename}")
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {dialog_id}: {e}")

    # –£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ –ë–î
    placeholders = ','.join('?' * len(dialog_ids))
    cursor.execute(f"DELETE FROM utterance_embeddings WHERE utterance_id IN (SELECT id FROM utterances WHERE dialog_id IN ({placeholders}))", dialog_ids)
    cursor.execute(f"DELETE FROM utterances WHERE dialog_id IN ({placeholders})", dialog_ids)
    cursor.execute(f"DELETE FROM dialogs WHERE id IN ({placeholders})", dialog_ids)
    conn.commit()
    conn.close()

    print("‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –∫ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ. –ó–∞–ø—É—Å—Ç–∏ pipeline.py —Å–Ω–æ–≤–∞.")

if __name__ == "__main__":
    find_and_retry()

===== FILE: utils.py =====
# utils.py
"""–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞."""

import logging
import config

def setup_logger(name: str, log_file: str, level=logging.INFO):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ñ–∞–π–ª –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–æ—Ä–º–∞—Ç–∞."""
    formatter = logging.Formatter(config.PIPELINE_LOG_FORMAT)
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(file_handler)
    
    # –ß—Ç–æ–±—ã –ª–æ–≥–∏ –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–ª–∏—Å—å
    logger.propagate = False
    
    return logger