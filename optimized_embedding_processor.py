"""–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è RTX 3060 12GB."""

import torch
import gc
import logging
from typing import List, Dict, Any, Optional, Generator
from sentence_transformers import SentenceTransformer
import numpy as np
import sqlite3
from pathlib import Path
import config
from utils import get_db_connection
import time

logger = logging.getLogger(__name__)

class OptimizedEmbeddingProcessor:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU –∏ –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    
    def __init__(self, model_name: str = None, batch_size: int = 32, max_length: int = 2048):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        self.model_name = model_name or config.EMBEDDING_MODEL_NAME
        self.batch_size = batch_size
        self.max_length = max_length
        self.model = None
        self.device = None
        self.conn = None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self._setup_device()
        
    def _setup_device(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        if torch.cuda.is_available():
            self.device = "cuda"
            # –û—á–∏—â–∞–µ–º –∫—ç—à CUDA
            torch.cuda.empty_cache()
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name()}")
            logger.info(f"üìä –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            self.device = "cpu"
            logger.info("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    
    def _load_model_lazy(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        if self.model is None:
            try:
                logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name} –Ω–∞ {self.device}...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
                self.model = SentenceTransformer(
                    self.model_name, 
                    device=self.device,
                    cache_folder='./model_cache'  # –ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–∏
                )
                
                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                self.model.max_seq_length = self.max_length
                
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è GPU
                if self.device == "cuda":
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                    if hasattr(torch.cuda, 'amp'):
                        self.model.half()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω mixed precision (float16)")
                    
                    # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                    torch.cuda.empty_cache()
                
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –Ω–∞ {self.device}")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                # Fallback –Ω–∞ CPU
                if self.device == "cuda":
                    logger.warning("‚ö†Ô∏è Fallback –Ω–∞ CPU...")
                    self.device = "cpu"
                    torch.cuda.empty_cache()
                    return self._load_model_lazy()
                else:
                    raise e
    
    def _unload_model(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏."""
        if self.model is not None:
            del self.model
            self.model = None
            gc.collect()
            if self.device == "cuda":
                torch.cuda.empty_cache()
            logger.info("üóëÔ∏è –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏")
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏."""
        memory_info = {}
        
        if self.device == "cuda" and torch.cuda.is_available():
            memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / 1024**3
            memory_info['gpu_reserved'] = torch.cuda.memory_reserved() / 1024**3
            memory_info['gpu_total'] = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        return memory_info
    
    def _log_memory_usage(self, stage: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
        memory = self._get_memory_usage()
        if self.device == "cuda":
            logger.info(f"üíæ {stage} - GPU: {memory['gpu_allocated']:.2f}GB/{memory['gpu_total']:.2f}GB")
        else:
            logger.info(f"üíæ {stage} - CPU —Ä–µ–∂–∏–º")
    
    def process_texts_batch(self, texts: List[str]) -> np.ndarray:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –ú–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if not texts:
            return np.array([])
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self._load_model_lazy()
        
        try:
            self._log_memory_usage("–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
            all_embeddings = []
            
            for i in range(0, len(texts), self.batch_size):
                batch_texts = texts[i:i + self.batch_size]
                
                logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//self.batch_size + 1}/{(len(texts) + self.batch_size - 1)//self.batch_size}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
                batch_embeddings = self.model.encode(
                    batch_texts,
                    batch_size=min(self.batch_size, len(batch_texts)),
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                )
                
                all_embeddings.append(batch_embeddings)
                
                # –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                del batch_embeddings
                gc.collect()
                
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                self._log_memory_usage(f"–ü–æ—Å–ª–µ –±–∞—Ç—á–∞ {i//self.batch_size + 1}")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            result = np.vstack(all_embeddings)
            
            self._log_memory_usage("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞: {e}")
            raise e
    
    def process_dialogs_batch(self, dialog_ids: List[str], conn: sqlite3.Connection) -> Dict[str, np.ndarray]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –±–∞—Ç—á–∞–º–∏.
        
        Args:
            dialog_ids: –°–ø–∏—Å–æ–∫ ID –¥–∏–∞–ª–æ–≥–æ–≤
            conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {dialog_id: embedding}
        """
        if not dialog_ids:
            return {}
        
        logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(dialog_ids)} –¥–∏–∞–ª–æ–≥–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ {self.batch_size}")
        
        cursor = conn.cursor()
        results = {}
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self._load_model_lazy()
            
            for i in range(0, len(dialog_ids), self.batch_size):
                batch_ids = dialog_ids[i:i + self.batch_size]
                
                logger.info(f"üìä –ë–∞—Ç—á {i//self.batch_size + 1}/{(len(dialog_ids) + self.batch_size - 1)//self.batch_size}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–∏–∞–ª–æ–≥–æ–≤
                placeholders = ','.join('?' * len(batch_ids))
                cursor.execute(f"""
                    SELECT id, text FROM dialogs 
                    WHERE id IN ({placeholders})
                """, batch_ids)
                
                batch_data = cursor.fetchall()
                
                if not batch_data:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
                batch_texts = [text for _, text in batch_data]
                batch_dialog_ids = [dialog_id for dialog_id, _ in batch_data]
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                batch_embeddings = self.process_texts_batch(batch_texts)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for j, dialog_id in enumerate(batch_dialog_ids):
                    results[dialog_id] = batch_embeddings[j]
                
                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                del batch_texts, batch_embeddings
                gc.collect()
                
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                self._log_memory_usage(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {i//self.batch_size + 1}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤: {e}")
            raise e
        finally:
            # –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self._unload_model()
    
    def process_utterances_batch(self, utterance_ids: List[str], conn: sqlite3.Connection) -> Dict[str, np.ndarray]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–ø–ª–∏–∫ –±–∞—Ç—á–∞–º–∏.
        
        Args:
            utterance_ids: –°–ø–∏—Å–æ–∫ ID —Ä–µ–ø–ª–∏–∫
            conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {utterance_id: embedding}
        """
        if not utterance_ids:
            return {}
        
        logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(utterance_ids)} —Ä–µ–ø–ª–∏–∫ –±–∞—Ç—á–∞–º–∏ –ø–æ {self.batch_size}")
        
        cursor = conn.cursor()
        results = {}
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self._load_model_lazy()
            
            for i in range(0, len(utterance_ids), self.batch_size):
                batch_ids = utterance_ids[i:i + self.batch_size]
                
                logger.info(f"üìä –ë–∞—Ç—á {i//self.batch_size + 1}/{(len(utterance_ids) + self.batch_size - 1)//self.batch_size}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Ä–µ–ø–ª–∏–∫
                placeholders = ','.join('?' * len(batch_ids))
                cursor.execute(f"""
                    SELECT id, text FROM utterances 
                    WHERE id IN ({placeholders})
                """, batch_ids)
                
                batch_data = cursor.fetchall()
                
                if not batch_data:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
                batch_texts = [text for _, text in batch_data]
                batch_utterance_ids = [utterance_id for utterance_id, _ in batch_data]
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                batch_embeddings = self.process_texts_batch(batch_texts)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for j, utterance_id in enumerate(batch_utterance_ids):
                    results[utterance_id] = batch_embeddings[j]
                
                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                del batch_texts, batch_embeddings
                gc.collect()
                
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                self._log_memory_usage(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {i//self.batch_size + 1}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–ø–ª–∏–∫: {e}")
            raise e
        finally:
            # –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self._unload_model()
    
    def save_embeddings_to_db(self, embeddings: Dict[str, np.ndarray], 
                             table_name: str, conn: sqlite3.Connection):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –ë–î.
        
        Args:
            embeddings: –°–ª–æ–≤–∞—Ä—å {id: embedding}
            table_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î
        """
        if not embeddings:
            return
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}")
        
        cursor = conn.cursor()
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            data_to_insert = []
            for item_id, embedding in embeddings.items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
                embedding_bytes = embedding.astype(np.float32).tobytes()
                data_to_insert.append((item_id, embedding_bytes))
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
            batch_size = 100
            for i in range(0, len(data_to_insert), batch_size):
                batch_data = data_to_insert[i:i + batch_size]
                
                if table_name == "embeddings":
                    cursor.executemany("""
                        INSERT OR REPLACE INTO embeddings (dialog_id, vector)
                        VALUES (?, ?)
                    """, batch_data)
                elif table_name == "utterance_embeddings":
                    cursor.executemany("""
                        INSERT OR REPLACE INTO utterance_embeddings (utterance_id, vector)
                        VALUES (?, ?)
                    """, batch_data)
                
                conn.commit()
            
            logger.info(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {table_name}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise e
    
    def get_optimal_batch_size(self) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        if self.device == "cuda":
            # –î–ª—è RTX 3060 12GB –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory >= 12:  # RTX 3060 12GB
                return 64
            elif gpu_memory >= 8:  # RTX 3070/3080 8GB
                return 32
            else:  # –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
                return 16
        else:
            # –î–ª—è CPU
            return 16
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        self._unload_model()

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    logging.basicConfig(level=logging.INFO)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = OptimizedEmbeddingProcessor(
        batch_size=32,  # –ù–∞—á–Ω–µ–º —Å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        max_length=2048
    )
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    test_texts = [
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        "–£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å",
        "–ö–æ–≥–¥–∞ –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤–æ?",
        "–•–æ—Ä–æ—à–æ, –ø–æ–Ω—è–ª"
    ]
    
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    
    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–æ–≤
        embeddings = processor.process_texts_batch(test_texts)
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(embeddings)} —Ç–µ–∫—Å—Ç–æ–≤")
        logger.info(f"üìä –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {embeddings.shape}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        memory = processor._get_memory_usage()
        if processor.device == "cuda":
            logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {memory['gpu_allocated']:.2f}GB")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

if __name__ == "__main__":
    main()
